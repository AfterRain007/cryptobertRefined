# -*- coding: utf-8 -*-
"""Augmemtatiooonn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yM-a-UZSkV4QAb5IfmNFkczgXfWrptyS

# Starting Out
"""

import numpy as np
import pandas as pd
!git clone https://github.com/AfterRain007/Skripsi
!ls
import sys

!pip install google-cloud-translate -q
from google.cloud import translate_v2
import time as time
import os

# from google.colab import drive
import shutil
# drive.mount('/content/drive')

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r"Skripsi/Trash/keyME.json"
# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r"Skripsi/ordinal-tractor-403615-0e2a1e3f1b66.json"

translate_client = translate_v2.Client()

dfTrain  = pd.read_csv("Skripsi/manualCleanAF/dfTrain.csv", usecols = ['text', 'sen'])

"""# Augmenting

## Google Translate
"""

def translateBby(text, lan):
  temp = translate_client.translate(text,lan)['translatedText']

  return translate_client.translate(temp,'en')['translatedText']

langList = ['ar', 'hi', 'bn', 'pt', 'tl', 'ko']
# List of language and it's code
# en = English
# ar = Arabic
# hi = Hindi
# bn = Bengali
# pt = Portuguese
# tl = Tagalog
# ko = Korean
Albanian

langList2 = ['zh', 'es', 'ru', 'ja', 'de', 'fr', 'it', 'id']
# zh = Chinese
# es = Spanish
# ru = Russia (with love)
# ja = Japan
# de = German
# fr = French
# it = Italian
# id = Indonesia

langList3 = ['af', 'pl', "hr", 'nl', 'gl', 'el', 'lv']

# af = Afrikaans
# pl = Polish
# hr = Croatioan
# nl = Dutch
# gl = Galician
# el = Greek
# lv = Latvian

langListAll = langList + langList2 + langList3

translateBby('testing one two three', 'id ')

frlngLast = ["sv", "da", "bg", "fi",
             "no", "sk", "cs", "sq"]

timeIt = []
for x in frlngLast:
  start = time.time()
  print(f'Staring-{x}')
  temp = dfTrain['text'].apply(translateBby, args=(x,))
  temp = pd.DataFrame(temp)
  temp['sen'] = dfTrain['sen']
  temp.to_csv(f'dfTrain-{x}GT.csv', index = False)
  shutil.copy(f'dfTrain-{x}GT.csv', '/content/drive/My Drive/kntl/')
  took = time.time() - start
  timeIt.append(took)
  timeit = pd.DataFrame(timeIt)
  timeit.to_csv(f"timeit.csv", index = False)
  shutil.copy(f'timeit.csv', '/content/drive/My Drive/kntl/')
  print(f"Augmention for {x} is done in {took} second \n")

dfTrain.reset_index(inplace = True, drop = True)

time.sleep(10000)

text = dfTrain['text'][45]
print(text)
translateBby(text, "sq")

translateBby("Hello there, my name is Vsauce", 'cs')

time.sleep(3600)

dfTrain

"""## Transformer"""

# Load model directly
!pip install transformers -q
!pip install sentencepiece -q
!pip install sacremoses -q
!pip install --upgrade tensorflow -q
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from google.colab import drive
import shutil
import torch
drive.mount('/content/drive')

import numpy as np
import pandas as pd
!git clone https://github.com/AfterRain007/Skripsi
!ls
import sys

import time as time

dfTrain = pd.read_csv("Skripsi/manualCleanAF/dfTrain.csv", usecols = ['text', 'sen'])

frlng = ['zh', 'es', 'ru', 'jap', 'de', 'fr', 'it', 'id']
# zh  = Chinese
# es  = Espanol
# ru  = Russia (with love)
# jap = Japan
# de  = German
# fr  = France
# it  = Italian
# id  = Indonesia

def translate(text):
  input_ids = tokenizer1(text, return_tensors="pt").input_ids.to(device)
  # print(len(input_ids))
  if len(input_ids[0]) > 512:
    return "Runtime 512 Error here baby!"
  else:
    outputs = model1.generate(input_ids=input_ids, num_beams=5)
    text1 = tokenizer1.batch_decode(outputs, skip_special_tokens=True)[0]

    input_ids = tokenizer2(text1, return_tensors="pt").input_ids.to(device)

    if len(input_ids[0]) < 512:
      outputs = model2.generate(input_ids=input_ids, num_beams=5)
      text2 = tokenizer2.batch_decode(outputs, skip_special_tokens=True)[0]
      return text2
    else:
      return "Runtime 512 Error here baby!"

def translate(text):
  input_ids = tokenizer1(text, return_tensors="pt").input_ids.to(device)
  # print(len(input_ids))
  if len(input_ids[0]) <= 512:
    outputs = model1.generate(input_ids=input_ids, num_beams=5)
    text1 = tokenizer1.batch_decode(outputs, skip_special_tokens=True)[0]

    input_ids = tokenizer2(text1, return_tensors="pt").input_ids.to(device)

    if len(input_ids[0]) < 512:
      outputs = model2.generate(input_ids=input_ids, num_beams=5)
      text2 = tokenizer2.batch_decode(outputs, skip_special_tokens=True)[0]
      return text2
    else:
      continue
  else:
    continue

crypto_vocabulary = [
    "satoshi", "halving", "mining", "block reward", "hashrate", "utxo", "soft fork", "hard fork",
    "cryptocurrency", "altcoin", "ico", "token", "wallet", "exchange", "fiat", "decentralization",
    "blockchain", "smart contract", "consensus algorithm", "node", "peer-to-peer", "distributed ledger", "merkle tree", "immutable",
    "market cap", "bull market", "bear market", "hodl", "fomo", "fud", "ath", "dex",
    "private key", "public key", "multi-signature", "cold wallet", "2fa", "privacy coin", "anonymity set", "zero-knowledge proof",
    "kyc", "aml", "sec", "ico regulations", "compliance",
    "lightning network", "segregated witness", "schnorr signatures", "sharding", "atomic swap", "layer 2 scaling solutions",
    "bitcoin maximalist", "crypto community", "whales", "pump and dump", "tokenomics", "cryptocurrency evangelist", "hodler community",
    "bitcoin", "ethereum", "ripple", "litecoin", "cardano", "bitcoin cash", "dogecoin", "binance coin", "stellar", "eos",  # More coin acronyms
    "tezos", "chainlink", "vechain", "dash", "neo", "tron", "cosmos", "monero", "iota", "tether",  # Even more coin acronyms
    "nem", "stellite", "algorand", "ethereum classic", "maker", "zcash", "crypto.com", "holo", "huobi token", "omisego",  # Additional coin acronyms
    "verge", "bitcoin gold", "0x", "aeternity", "augur", "lisk", "bitcoin diamond", "icon", "bittorrent", "siacoin",  # More coin acronyms
    "waves", "electroneum", "theta", "basic attention token", "dock", "mithril", "nano", "qtum", "internet computer", "ravencoin",  # Even more coin acronyms
    "btc", "eth", "xrp", "ltc", "ada", "bch", "doge", "bnb", "xlm", "eos",  # More coin acronyms
    "xtz", "link", "vet", "dash", "neo", "trx", "atom", "xmr", "iota", "usdt",  # Even more coin acronyms
    "xem", "xtl", "algo", "etc", "mkr", "zec", "cro", "hot", "ht", "omg",  # Additional coin acronyms
    "xvg", "btg", "zrx", "ae", "rep", "lsk", "bcd", "icx", "btt", "sc",  # More coin acronyms
    "waves", "etn", "theta", "bat", "dock", "mith", "nano", "qtum", "icp", "rvn"  # Even more coin acronyms
]

device = "cuda:0" if torch.cuda.is_available() else "cpu"

timeIt = []
for x in frlng:
  tokenizer1  = AutoTokenizer.from_pretrained(f"Helsinki-NLP/opus-mt-en-{x}")
  tokenizer1.additional_special_tokens = crypto_vocabulary
  model1      = AutoModelForSeq2SeqLM.from_pretrained(f"Helsinki-NLP/opus-mt-en-{x}")
  model1      = model1.to(device)
  tokenizer2  = AutoTokenizer.from_pretrained(f"Helsinki-NLP/opus-mt-{x}-en")
  tokenizer2.additional_special_tokens = crypto_vocabulary
  model2      = AutoModelForSeq2SeqLM.from_pretrained(f"Helsinki-NLP/opus-mt-{x}-en")
  model2      = model2.to(device)
  torch.cuda.empty_cache()

  print(f"Starting augmention for {x}")
  start = time.time()
  temp = dfTrain['text'].apply(translate)
  temp = pd.DataFrame(temp)
  temp['sen'] = dfTrain['sen'].copy()
  temp.to_csv(f"dfTrain-{x}T.csv", index = False)
  shutil.copy(f'dfTrain-{x}T.csv', '/content/drive/My Drive/dfTrain/')
  took = time.time() - start
  timeIt.append(took)
  timeit = pd.DataFrame(timeIt)
  timeit.to_csv(f"timeit2.csv", index = False)
  shutil.copy(f'timeit2.csv', '/content/drive/My Drive/dfTrain/')
  print(f"Done in {took} seconds")

"""## Transformer Again"""

# Load model directly
!pip install transformers -q
!pip install sentencepiece -q
!pip install sacremoses -q
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

device = "cuda:0" if torch.cuda.is_available() else "cpu"
tokenizer1 = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
tokenizer2 = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
model = model.to(device)

langList = [
    "de_DE",  # German
    "cs_CZ",  # Czech
    "fr_XX",  # French
    "ja_XX",  # Japanese
    "es_XX",  # Spanish
    "ru_RU",  # Russian
    "pl_PL",  # Polish
    "zh_CN",  # Chinese
    "fi_FI",  # Finnish
    "lv_LV",  # Latvian
    "lt_LT",  # Lithuanian
    "hi_IN",  # Hindi
    "et_EE"   # Esonian
]

def translateThis(text):
  encoded = tokenizer1(text, return_tensors="pt").to(device)
  generated_tokens = model.generate(**encoded, forced_bos_token_id=tokenizer1.lang_code_to_id[tokenizer2.src_lang])
  text = tokenizer1.batch_decode(generated_tokens, skip_special_tokens=True)[0]

  encoded = tokenizer2(text, return_tensors="pt").to(device)
  generated_tokens = model.generate(**encoded, forced_bos_token_id=tokenizer2.lang_code_to_id[tokenizer1.src_lang])
  return tokenizer2.batch_decode(generated_tokens, skip_special_tokens=True)[0]

from google.colab import drive
import shutil
drive.mount('/content/drive')

tokenizer1.src_lang = 'en_XX'
for x in langList:
  tokenizer2.src_lang = x
  start = time.time()
  print(f'Staring-{x}')
  temp = df['text'].apply(translateThis)
  temp = pd.DataFrame(temp)
  temp['sen'] = df['sen']
  temp.to_csv(f'dfTrain_{x}.csv')
  shutil.copy(f'dfTrain_{x}.csv', '/content/drive/My Drive/')
  print(f"Augmention for {x} is done in {time.time() - start} second \n")