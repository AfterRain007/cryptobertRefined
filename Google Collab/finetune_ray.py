# -*- coding: utf-8 -*-
"""FineTune Ray.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RX6WI2F75nhBtiSonGMPD8mkLtceHAw6

#Install Indenpenciesiseaus
"""

!pip install transformers -q
!pip install sentencepiece -q
!pip install sacremoses -q
!pip install --upgrade tensorflow -q

!pip install transformers[torch] -q
!pip install accelerate -U -q

!pip install ray[tune]==2.6.3
!pip install emoji==0.6.0 -q
!pip install tensorflow tf2onnx -q

#Restart Runtime After Running This

"""# Load Indenpenciesiseaus"""

# Load model directly
from google.colab import drive
drive.mount('/content/drive')
import shutil
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, AutoModelForSequenceClassification
from transformers import Trainer, TrainingArguments, EarlyStoppingCallback
import os
from google.colab import files
import json
import random
import numpy as np
import pandas as pd
!git clone https://github.com/AfterRain007/Skripsi
!ls
import sys
import time as time
from transformers import set_seed

"""#Actual Dataset"""

GtH_eda = pd.read_csv('/content/Skripsi/Augment Finale/eda_dfTrainGtH.txt', sep="\t", header = None)
GtH_eda.columns = ['sen', 'text']
GtH_eda.head()

eda = pd.read_csv('/content/Skripsi/Augment Finale/eda_dfTrain.txt', sep="\t", header = None)
eda.columns = ['sen', 'text']
eda.head()

#Google Translate Data

DIR = "Skripsi/AugmentationGT/"

frlng = ['it', 'fr', "sv", "da", 'pt', 'id', 'pl', 'hr', "bg", "fi",
         "no", 'ru', 'es', 'nl', 'af', 'de', "sk", "cs", 'lv', "sq"]
        #  'ko', 'zh', 'tl' 'gl', 'el', 'ar', 'hi', 'ja']

dfGt = pd.DataFrame()
for x in frlng:
  temp = pd.read_csv(DIR + f"dfTrain-{x}GT.csv")
  dfGt = pd.concat([dfGt, temp])
dfGt.reset_index(inplace = True, drop =  True)

dfGtH = dfGt[:int(len(dfGt)/2)].copy()

#Transformer Data

frlng2 = ['zh', 'es', 'ru', 'jap', 'de', 'fr', 'it', 'id']

DIR = "Skripsi/AugmentationT/"

dfT = pd.DataFrame()
for x in frlng2:
  temp = pd.read_csv(DIR + f"dfTrain-{x}T.csv")
  dfT = pd.concat([dfT, temp])

dfT.reset_index(inplace = True, drop =  True)

dfT = dfT[~dfT['text'].str.contains("- I'm sorry. - I'm sorry.") &
          ~dfT['text'].str.contains("Runtime 512 Error here baby!") &
          ~dfT['text'].str.contains("The Bitcoin Treasure is worth our bitcoin.")]

dfT.reset_index(inplace = True, drop = True)

#Import all the module
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import torch
from html import unescape

#Delete """" if you haven't download

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

#Delete """" if you haven't download

# Compile the regular expressions outside the clean_text function
url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
ftp_pattern = re.compile(r'ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')

# Set of punctuation characters
punctuation_set = set(string.punctuation)

def clean_text(text):
    t = re.sub(url_pattern, ' ', text)  # remove urls if any
    t = re.sub(ftp_pattern, ' ', t)  # remove urls if any
    t = unescape(t)  # html entities fix

    # Convert text to lowercase
    text = t.lower()

    # Remove punctuation
    text = ''.join(char for char in text if char not in punctuation_set)

    # Tokenization
    tokens = word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words("english"))
    tokens = [token for token in tokens if token not in stop_words]

    # Remove special characters and numbers
    tokens = [re.sub(r"[^a-zA-Z]", "", token) for token in tokens]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Remove unnecessary spaces
    tokens = [token.strip() for token in tokens if token.strip()]

    # Join tokens back into a single string
    cleaned_text = " ".join(tokens)

    return cleaned_text

# Example usage
text = "This is an example sentence! 123# https://www.youtube.com/watch?v=_9bw_VtMUGA"
cleaned_text = clean_text(text)
print(cleaned_text)

def has_repeating_word(text, repetition_threshold=3):
    # Find all words in the text
    words = re.findall(r'\b\w+\b', text.lower())

    # Create a regular expression pattern for detecting repeating words
    pattern = r'\b(\w+)' + r'(\s+\1){%d,}\b' % (repetition_threshold - 1)

    # Search for the pattern in the text
    match = re.search(pattern, ' '.join(words))

    # Return True if a match is found, indicating repeating words
    return match is not None

dfVal = pd.read_csv("Skripsi/manualCleanAF/dfVal.csv")
dfTest = pd.read_csv("Skripsi/manualCleanAF/dfTest.csv")

# DataFrame dfVal
dfVal['sen']  = dfVal['sen'] + 1

# DataFrame dfTest
dfTest['sen'] = dfTest['sen'] + 1

dfTrain = pd.read_csv("Skripsi/manualCleanAF/dfTrain.csv", usecols = ['text', 'sen'])

def preprocess_dataframe(df, spammy):
  df['text'] = df['text'].astype(str)
  df['text'] = df['text'].apply(clean_text)
  df['WC']   = df['text'].apply(lambda x: len(x.split()))
  df['spam'] = df['text'].apply(has_repeating_word, args=(spammy,))
  df = df[(df['WC'] >= 4) & (df['spam'] == False)]
  df.drop(['WC', 'spam'], axis = 1, inplace = True)
  df = pd.concat([dfTrain, df])
  df.drop_duplicates(subset=['text'], keep=False).reset_index(drop = True, inplace = True)

  minLen = len(df[df['sen'] == -1])
  df = df[df['sen'].isin([-1, 0, 1])].groupby('sen').head(minLen)
  df.reset_index(inplace = True, drop = True)

  df['sen'] = df['sen'] + 1
  return df['text'].tolist(), df['sen'].tolist()

trainGt_texts, trainGt_labels   = preprocess_dataframe(dfGt, 5)
trainGtH_texts, trainGtH_labels = preprocess_dataframe(dfGtH, 5)
trainT_texts, trainT_labels     = preprocess_dataframe(dfT, 3)
trainEDA_texts, trainEDA_labels   = preprocess_dataframe(eda, 5)
trainEDA2_texts, trainEDA2_labels = preprocess_dataframe(GtH_eda, 5)


trainPure_texts = dfTrain['text'].tolist()
dfTrain['sen'] = dfTrain['sen'] + 1
trainPure_labels = dfTrain['sen'].tolist()

val_texts = dfVal['text'].tolist()
test_texts = dfTest['text'].tolist()
val_labels = dfVal['sen'].tolist()
test_labels = dfTest['sen'].tolist()

print(f"Amount of Training Google Translate Data      : {len(trainGt_texts)}")
print(f"Amount of Training Google Translate Half Data : {len(trainGtH_texts)}")
print(f"Amount of Training Transformer Data           : {len(trainT_texts)}")
print(f"Amount of Pure Training Data                  : {len(trainPure_texts)}")
print(f"Amount of EDA Training Data                   : {len(trainEDA_texts)}")
print(f"Amount of GTH EDA Training Data               : {len(trainEDA2_texts)}\n")
print(f"Amount of Validation Data                     : {len(dfVal)}")
print(f"Amount of Test Data                           : {len(dfTest)}")

f = open("Skripsi/cryptoVocab.txt", "r")
crypto_vocabulary = f.read().split(',')
crypto_vocabulary = [term.replace('"', '') for term in crypto_vocabulary]

class dataSet(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

"""#Final Prep"""

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

def compute_metrics(p):
    pred, labels = p
    pred = np.argmax(pred, axis=1)

    accuracy = accuracy_score(y_true=labels, y_pred=pred)
    recall = recall_score(y_true=labels, y_pred=pred, average='macro')  # Set average to 'micro', 'macro', or 'weighted'
    precision = precision_score(y_true=labels, y_pred=pred, average='macro')  # Set average to 'micro', 'macro', or 'weighted'
    f1 = f1_score(y_true=labels, y_pred=pred, average='macro')  # Set average to 'micro', 'macro', or 'weighted'

    return {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}

def dataPrep(data, token):
  global tokenizer, train_dataset, val_dataset, test_dataset

  if data == 0:
    train_texts = trainGt_texts
    train_labels = trainGt_labels
  elif data == 1:
    train_texts = trainGtH_texts
    train_labels = trainGtH_labels
  elif data == 2:
    train_texts = trainT_texts
    train_labels = trainT_labels
  elif data == 3:
    train_texts = trainPure_texts
    train_labels = trainPure_labels
  elif data == 4:
    train_texts = trainEDA_texts
    train_labels = trainEDA_labels
  else:
    train_texts = trainEDA2_texts
    train_labels = trainEDA2_labels

  if (token == "cardiffnlp/twitter-roberta-base-sentiment-latest") | (token == "cardiffnlp/twitter-xlm-roberta-base-sentiment"):
    size = 256
  else:
    size = 128

  tokenizer = AutoTokenizer.from_pretrained(token)
  # tokenizer.additional_special_tokens = crypto_vocabulary

  try:
    new_tokens = set(crypto_vocabulary) - set(tokenizer.vocab.keys())
  except:
    new_tokens = set(crypto_vocabulary) - set(tokenizer.get_vocab().keys())

  tokenizer.add_tokens(list(new_tokens))

  train_encodings = tokenizer(train_texts, max_length = size, truncation=True, padding=True)
  val_encodings = tokenizer(val_texts, max_length = size, truncation=True, padding=True)
  test_encodings = tokenizer(test_texts, max_length = size, truncation=True, padding=True)

  train_dataset = dataSet(train_encodings, train_labels)
  val_dataset = dataSet(val_encodings, val_labels)
  test_dataset = dataSet(test_encodings, test_labels)

def sentiment_score(review, model):
    # Tokenize the review outside the loop
    tokens = tokenizer(review, return_tensors='pt', max_length = 128, truncation = True).input_ids.to(device)

    # Pass the tokens directly to the model for batch processing
    result = model(tokens)

    # Convert the tensor to a numpy array and extract the predicted sentiment
    sentiment = int(torch.argmax(result.logits))

    return sentiment

def testing(text):
  encoding = tokenizer(text, return_tensors="pt", max_length = 128, truncation=True)
  encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}

  outputs = trainer.model(**encoding)

  logits = outputs.logits

  return int(torch.argmax(logits).cpu().numpy())

#Choose your fighter!

fighterList = ["cardiffnlp/twitter-roberta-base-sentiment-latest", #0
               "finiteautomata/bertweet-base-sentiment-analysis",  #1
               "ElKulako/cryptobert",                              #2
               "cardiffnlp/twitter-xlm-roberta-base-sentiment"     #3
               ]

"""# Actual Training Grid Search

## Griddy
"""

df = pd.read_csv('./Skripsi/gridSearchNLP/paramsSenFinale.csv')
df.set_index('type', inplace = True)
df.head()

df.iloc[7]

from transformers import set_seed
index_mapping = {
    '256GT': (0, 0),
    '256GTH': (0, 1),
    '256T': (0, 2),
    '128GT': (1, 0),
    '128GTH': (1, 1),
    '128T': (1, 2),
    '128_2GT': (2, 0),
    '128_2GTH': (2, 1),
    '128_2T': (2, 2),
}

seed = 69420

try:
  resultEval = pd.read_csv("./resultEval.csv")
  result = resultEval.to_dict(orient='records')
  lenRus = len(resultEval)
except:
  result = []
  lenRus = 0

#0,2,5,6,7,9,14,17,19
BEST = df.iloc[[7]]

for i, x in enumerate(BEST.index):
  set_seed(seed)
  torch.manual_seed(seed)
  torch.cuda.manual_seed(seed)
  np.random.seed(seed)
  random.seed(seed)

  fighter_index, data_index = index_mapping[x]
  fighter = fighterList[fighter_index]
  print(f"{fighter}-{i}")
  dataPrep(data_index, fighter)

  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  training_args = TrainingArguments(
      output_dir=f'./results{17}',
      num_train_epochs=3,
      per_device_train_batch_size=16,
      per_device_eval_batch_size=32,
      learning_rate=0.0001,
      warmup_steps=float(BEST['warmup_steps'][i]),
      weight_decay=float(BEST['weight_decay'][i]),
      logging_dir='./logs',
      logging_steps=10,
      seed=69420,
      load_best_model_at_end=True,
      evaluation_strategy="steps",
      eval_steps=100,
      metric_for_best_model = 'f1',
      save_steps = 100,
  )

  model = AutoModelForSequenceClassification.from_pretrained(fighter)
  model.to(device)
  model.resize_token_embeddings(len(tokenizer))
  torch.cuda.empty_cache()

  trainer = Trainer(
      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
      args=training_args,                  # training arguments, defined above
      train_dataset=train_dataset,         # training dataset
      eval_dataset=val_dataset,            # evaluation dataset
      compute_metrics=compute_metrics,
      callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]
  )

  trainer.train()

  dfTest['sen_2'] = dfTest['text'].apply(testing)
  same_values_count = (dfTest['sen'] == dfTest['sen_2']).sum()
  Accuracy = same_values_count/300

  if Accuracy > .675:
    print(f"HELL YEAH WE GOT ONE ON NUMBER-{i}")
    shutil.make_archive(f'results{i}', 'zip', f'./results{i}')
    shutil.copy(f'results{i}.zip',f'/content/drive/My Drive/')

  evaluation = trainer.evaluate()
  evaluation['modelName'] = x
  evaluation['testAccuracy'] = Accuracy
  result.append(evaluation)

  resultEval = pd.DataFrame(result)
  resultEval.to_csv("resultEval.csv", index = False)
  shutil.copy(f'resultEval.csv', '/content/drive/My Drive/')

from transformers import set_seed
index_mapping = {
    '256GT': (0, 0),
    '256GTH': (0, 1),
    '256T': (0, 2),
    '128GT': (1, 0),
    '128GTH': (1, 1),
    '128T': (1, 2),
    '128_2GT': (2, 0),
    '128_2GTH': (2, 1),
    '128_2T': (2, 2),
}

seed = 69420

try:
  resultEval = pd.read_csv("./resultEval.csv")
  result = resultEval.to_dict(orient='records')
  lenRus = len(resultEval)
except:
  result = []
  lenRus = 0

#0,2,5,6,7,9,14,17,19
BEST = df.iloc[[19]]

for i, x in enumerate(BEST.index):
  set_seed(seed)
  torch.manual_seed(seed)
  torch.cuda.manual_seed(seed)
  np.random.seed(seed)
  random.seed(seed)

  fighter_index, data_index = index_mapping[x]
  fighter = fighterList[fighter_index]
  print(f"{fighter}-{i}")
  dataPrep(data_index, fighter)

  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  training_args = TrainingArguments(
      output_dir=f'./results{19}',
      num_train_epochs=3,
      per_device_train_batch_size=16,
      per_device_eval_batch_size=32,
      learning_rate=float(BEST['learning_rate'][i]),
      warmup_steps=float(BEST['warmup_steps'][i]),
      weight_decay=float(BEST['weight_decay'][i]),
      logging_dir='./logs',
      logging_steps=10,
      seed=69420,
      load_best_model_at_end=True,
      evaluation_strategy="steps",
      eval_steps=100,
      metric_for_best_model = 'f1',
      save_steps = 100,
  )

  model = AutoModelForSequenceClassification.from_pretrained(fighter)
  model.to(device)
  model.resize_token_embeddings(len(tokenizer))
  torch.cuda.empty_cache()

  trainer = Trainer(
      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
      args=training_args,                  # training arguments, defined above
      train_dataset=train_dataset,         # training dataset
      eval_dataset=val_dataset,            # evaluation dataset
      compute_metrics=compute_metrics,
      callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]
  )

  trainer.train()

  dfTest['sen_2'] = dfTest['text'].apply(testing)
  same_values_count = (dfTest['sen'] == dfTest['sen_2']).sum()
  Accuracy = same_values_count/300

  if Accuracy > .675:
    print(f"HELL YEAH WE GOT ONE ON NUMBER-{i}")
    shutil.make_archive(f'results{i}', 'zip', f'./results{i}')
    shutil.copy(f'results{i}.zip',f'/content/drive/My Drive/')

  evaluation = trainer.evaluate()
  evaluation['modelName'] = x
  evaluation['testAccuracy'] = Accuracy
  result.append(evaluation)

  resultEval = pd.DataFrame(result)
  resultEval.to_csv("resultEval.csv", index = False)
  shutil.copy(f'resultEval.csv', '/content/drive/My Drive/')

!rm -rf './results14/'

# !rm -rf './results18/'
i = 17
x = 800
shutil.make_archive(f'results{i}-{x}', 'zip', f'./results{i}/checkpoint-{x}')
shutil.copy(f'results{i}-{x}.zip',f'/content/drive/My Drive/')

shutil.copy(f'results{i}-{x}.zip',f'/content/drive/My Drive/')

from transformers import set_seed
index_mapping = {
    '256GT': (0, 0),
    '256GTH': (0, 1),
    '256T': (0, 2),
    '128GT': (1, 0),
    '128GTH': (1, 1),
    '128T': (1, 2),
    '128_2GT': (2, 0),
    '128_2GTH': (2, 1),
    '128_2T': (2, 2),
}

seed = 69420

try:
  resultEval = pd.read_csv("./resultEval.csv")
  result = resultEval.to_dict(orient='records')
  lenRus = len(resultEval)
except:
  result = []
  lenRus = 0

for i, x in enumerate(df.index):
  if i < lenRus:
    continue

  set_seed(seed)
  torch.manual_seed(seed)
  torch.cuda.manual_seed(seed)
  np.random.seed(seed)
  random.seed(seed)

  fighter_index, data_index = index_mapping[x]
  fighter = fighterList[fighter_index]
  print(fighter)
  dataPrep(data_index, fighter)

  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  training_args = TrainingArguments(
      output_dir=f'./results{i}',
      num_train_epochs=5,
      per_device_train_batch_size=16,
      per_device_eval_batch_size=32,
      learning_rate=float(df['learning_rate'][i]),
      warmup_steps=float(df['warmup_steps'][i]),
      weight_decay=float(df['weight_decay'][i]),
      logging_dir='./logs',
      logging_steps=10,
      seed=69420,
      load_best_model_at_end=True,
      evaluation_strategy="steps",
      eval_steps=100,
      metric_for_best_model = 'f1'
  )

  model = AutoModelForSequenceClassification.from_pretrained(fighter)
  model.to(device)
  model.resize_token_embeddings(len(tokenizer))
  torch.cuda.empty_cache()

  trainer = Trainer(
      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
      args=training_args,                  # training arguments, defined above
      train_dataset=train_dataset,         # training dataset
      eval_dataset=val_dataset,            # evaluation dataset
      compute_metrics=compute_metrics,
      callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]
  )

  trainer.train()

  dfTest['sen_2'] = dfTest['text'].apply(testing)
  same_values_count = (dfTest['sen'] == dfTest['sen_2']).sum()
  Accuracy = same_values_count/300

  if Accuracy > .675:
    print(f"HELL YEAH WE GOT ONE ON NUMBER-{i}")
    shutil.make_archive(f'results{i}', 'zip', f'./results{i}')
    shutil.copy(f'results{i}.zip',f'/content/drive/My Drive/')

  evaluation = trainer.evaluate()
  evaluation['modelName'] = x
  evaluation['testAccuracy'] = Accuracy
  result.append(evaluation)

  resultEval = pd.DataFrame(result)
  resultEval.to_csv("resultEval.csv", index = False)
  shutil.copy(f'resultEval.csv', '/content/drive/My Drive/')

!rm -rf './results18/'
i = 17
x = 1000
shutil.make_archive(f'results{i}', 'zip', f'./results{i}/checkpoint-{x}')
shutil.copy(f'results{i}.zip',f'/content/drive/My Drive/')

"""## Only Griddy"""

from transformers import set_seed
index_mapping = {
    '256GT': (0, 0),
    '256GTH': (0, 1),
    '256T': (0, 2),
    '128GT': (1, 0),
    '128GTH': (1, 1),
    '128T': (1, 2),
    '128_2GT': (2, 0),
    '128_2GTH': (2, 1),
    '128_2T': (2, 2),
}

seed = 69420
!rm -rf './results0/'
!rm -rf './results1/'

for i, x in enumerate(df.iloc[[9]].index):
  set_seed(seed)
  torch.manual_seed(seed)
  torch.cuda.manual_seed(seed)
  np.random.seed(seed)
  random.seed(seed)

  fighter_index, data_index = index_mapping[x]
  fighter = fighterList[fighter_index]
  print(fighter)
  dataPrep(data_index, fighter)

  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

  training_args = TrainingArguments(
      output_dir=f'./results{i}',
      num_train_epochs=5,
      per_device_train_batch_size=16,
      per_device_eval_batch_size=32,
      learning_rate=float(df['learning_rate'][i]),
      warmup_steps=float(df['warmup_steps'][i]),
      weight_decay=float(df['weight_decay'][i]),
      logging_dir='./logs',
      logging_steps=10,
      seed=69420,
      load_best_model_at_end=True,
      evaluation_strategy="steps",
      eval_steps=100,
      metric_for_best_model = 'f1',
      save_steps = 100
  )

  model = AutoModelForSequenceClassification.from_pretrained(fighter)
  model.to(device)
  model.resize_token_embeddings(len(tokenizer))
  torch.cuda.empty_cache()

  trainer = Trainer(
      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
      args=training_args,                  # training arguments, defined above
      train_dataset=train_dataset,         # training dataset
      eval_dataset=val_dataset,            # evaluation dataset
      compute_metrics=compute_metrics,
      callbacks = [EarlyStoppingCallback(early_stopping_patience=100)]
  )

  trainer.train()

  dfTest['sen_2'] = dfTest['text'].apply(testing)
  same_values_count = (dfTest['sen'] == dfTest['sen_2']).sum()
  Accuracy = same_values_count/300
  print(Accuracy)

df[df.index == "128_2T"].sort_values(by = 'eval_acc')

fighter = fighterList[2]
dataPrep(4, fighter)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

seed = 69420

set_seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

training_args = TrainingArguments(
    output_dir=f'./results',
    num_train_epochs=1,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=5e-5,
    warmup_steps=500,
    weight_decay=0.1,
    logging_dir='./logs',
    logging_steps=10,
    seed=69420,
    load_best_model_at_end=True,
    evaluation_strategy="steps",
    eval_steps=100,
    metric_for_best_model = 'f1',
    save_steps = 100,
    max_steps = 100
)

model = AutoModelForSequenceClassification.from_pretrained(fighter)
model.to(device)
model.resize_token_embeddings(len(tokenizer))
torch.cuda.empty_cache()

trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset,            # evaluation dataset
    compute_metrics=compute_metrics,
    callbacks = [EarlyStoppingCallback(early_stopping_patience=100)]
)

trainer.train()

dfTest['sen_2'] = dfTest['text'].apply(testing)
same_values_count = (dfTest['sen'] == dfTest['sen_2']).sum()
Accuracy = same_values_count/300

evaluation = trainer.evaluate()
evaluation['modelName'] = x
evaluation['testAccuracy'] = Accuracy

print(evaluation)

fighter = fighterList[2]
dataPrep(2, fighter)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

seed = 69420

set_seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

training_args = TrainingArguments(
    output_dir=f'./results',
    num_train_epochs=5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=5e-5,
    warmup_steps=500,
    weight_decay=0.1,
    logging_dir='./logs',
    logging_steps=10,
    seed=69420,
    load_best_model_at_end=True,
    evaluation_strategy="steps",
    eval_steps=100,
    metric_for_best_model = 'f1',
    save_steps = 100
)

model = AutoModelForSequenceClassification.from_pretrained(fighter)
model.to(device)
model.resize_token_embeddings(len(tokenizer))
torch.cuda.empty_cache()

trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset,            # evaluation dataset
    compute_metrics=compute_metrics,
    callbacks = [EarlyStoppingCallback(early_stopping_patience=100)]
)

trainer.train()

dfTest['sen_2'] = dfTest['text'].apply(testing)
same_values_count = (dfTest['sen'] == dfTest['sen_2']).sum()
Accuracy = same_values_count/300

evaluation = trainer.evaluate()
evaluation['modelName'] = x
evaluation['testAccuracy'] = Accuracy

print(evaluation)

from sklearn.model_selection import ParameterGrid

param_name = [
    "per_gpu_batch_size",
    "per_gpu_eval_size",
    "warmup_steps",
    "weight_decay",
    "learning_rate",
    "num_epochs"
]

# param_grid = {
#     param_name[0]: [16, 32],
#     param_name[1]: [16, 32],
#     param_name[2]: [250, 500],
#     param_name[3]: [0, 0.3],
#     param_name[4]: [1e-5, 3e-5],
#     param_name[5]: [3, 5]
# }

param_grid = {
    param_name[0]: [16, 32],
    param_name[1]: [16, 32],
    param_name[2]: [500],
    param_name[3]: [0.3],
    param_name[4]: [1e-5, 3e-5],
    param_name[5]: [3, 5]
}

# Create a list of dictionaries representing all combinations of parameters
param_combinations = list(ParameterGrid(param_grid))

#Grid Search
from transformers import Trainer, TrainingArguments, EarlyStoppingCallback
import random

# result = []
# result = pd.read_csv("drive/MyDrive/resultEval.csv", usecols=usedcol)
# Length = len(result)-1
# result = result[:Length]
# result = result.to_dict(orient='records')

seed = 69420
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Iterate through each combination and perform grid search
for i, params in enumerate(param_combinations):
  training_args = TrainingArguments(
      output_dir='./results',
      num_train_epochs=params['num_epochs'],
      per_device_train_batch_size=params['per_gpu_batch_size'],
      per_device_eval_batch_size=params['per_gpu_eval_size'],
      learning_rate=params['learning_rate'],
      warmup_steps=params['warmup_steps'],
      weight_decay=params['weight_decay'],
      logging_dir='./logs',
      logging_steps=10,
      seed=69420,
      load_best_model_at_end=True,
      evaluation_strategy="steps",
      eval_steps=100,
  )

  model = AutoModelForSequenceClassification.from_pretrained(fighter)

  print(f"Starting iteration-{i}")

  trainer = Trainer(
      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
      args=training_args,                  # training arguments, defined above
      train_dataset=train_dataset,         # training dataset
      eval_dataset=val_dataset,            # evaluation dataset
      compute_metrics=compute_metrics
  )

  evaluation = trainer.evaluate()

  dfTest['sen_2'] = dfTest['text'].apply(sentiment_score, args=(model, ))
  same_values_count = (dfTest['sen'] == dfTest['sen_2']).sum()
  Accuracy = same_values_count/300

  params.update(evaluation)
  params['Test Accuracy'] = Accuracy
  result.append(params)

  resultEval = pd.DataFrame(result)
  resultEval.to_csv("resultEvalGT.csv")
  shutil.copy(f'resultEvalGT.csv', '/content/drive/My Drive/Model71P/')

"""# Ray

## Preparation
"""

import ray
from ray import tune
from ray.tune import CLIReporter
from ray.tune.examples.pbt_transformers.utils import build_compute_metrics_fn
from ray.tune.schedulers import PopulationBasedTraining
from transformers import Trainer, TrainingArguments, EarlyStoppingCallback
import random

seed = 69420
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

task_name = "rte"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def get_model():
  model = AutoModelForSequenceClassification.from_pretrained(fighter)
  model.resize_token_embeddings(len(tokenizer))
  return model

training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=5e-5,  # config
    do_train=True,
    do_eval=True,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    num_train_epochs=3,
    max_steps=-1,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    warmup_steps=0,  # config
    weight_decay=0.1,  # config
    logging_dir="./logs",
    skip_memory_metrics=True,
    report_to="none",
)

tune_config = {
    "per_device_train_batch_size": 16,
    "per_device_eval_batch_size": 32,
}

scheduler = PopulationBasedTraining(
    time_attr="training_iteration",
    metric="eval_acc",
    mode="max",
    perturbation_interval=1,
    hyperparam_mutations={
        "weight_decay": tune.uniform(0.0, 0.3),
        "learning_rate": tune.uniform(1e-5, 5e-5),
        "warmup_steps" : tune.choice([0, 250, 500]),
    },
)

reporter = CLIReporter(
    parameter_columns={
        "weight_decay": "w_decay",
        "learning_rate": "lr",
        "per_device_train_batch_size": "train_bs/gpu",
        "num_train_epochs": "num_epochs",
    },
    metric_columns=["eval_acc", "eval_loss", "epoch", "training_iteration"],
)

def train_fn(config):
    checkpoint: train.Checkpoint = train.get_checkpoint()
    if checkpoint:
        with checkpoint.as_directory() as checkpoint_dir:
            torch.load(os.path.join(checkpoint_dir, "checkpoint.pt"))

def get_first_subfolder(path):
    subfolders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]
    if subfolders:
        return os.path.join(path, subfolders[0])
    else:
        return None

def tryTest(fileName):
  print("\nTesting the Water and Downloading Result")
  source_directory = "./ray_results/tune_transformer_pbt/"
  usedcols = ['eval_loss', 'eval_acc', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch', 'time_this_iter_s', 'training_iteration']
  folders = [f for f in os.listdir(source_directory) if os.path.isdir(os.path.join(source_directory , f))]

  tempDf = pd.DataFrame()

  for i, folder in enumerate(folders):
      source_file_path = os.path.join(source_directory, folder)

      with open(source_file_path+"/params.json", 'r') as f:
          data = json.load(f)

      temp = pd.read_csv(source_file_path+"/progress.csv", usecols = usedcols)
      temp2 = pd.DataFrame([data])

      source_file_path = get_first_subfolder(source_file_path)
      source_file_path = get_first_subfolder(source_file_path)

      model = AutoModelForSequenceClassification.from_pretrained(source_file_path)
      model.to(device)
      model.resize_token_embeddings(len(tokenizer))

      dfTest['sen_2'] = dfTest['text'].apply(sentiment_score, args=(model,))
      same_values_count = (dfTest['sen'] == dfTest['sen_2']).sum()
      Accuracy = same_values_count/len(dfTest)

      temp2['testAccuracy'] = Accuracy
      temp = pd.concat([temp, temp2], axis = 1)
      tempDf = pd.concat([tempDf, temp])

  destination_directory = "/content/drive/MyDrive/Result"

  if os.path.exists(destination_directory) == False:
      os.makedirs(destination_directory)

  tempDf.to_csv(f"result_{fileName}.csv")

  shutil.copy(f"result_{fileName}.csv", destination_directory)
  print(f"{fileName} is done El Capitan!")

def start():
  destination_directory = './ray_results'
  if os.path.exists(destination_directory):
      shutil.rmtree(destination_directory)
  ray.shutdown()
  torch.cuda.empty_cache()

  trainer = Trainer(
      model_init=get_model,
      args=training_args,
      train_dataset=train_dataset,
      eval_dataset=val_dataset,
      compute_metrics=build_compute_metrics_fn(task_name),
  )

  trainer.hyperparameter_search(
      hp_space=lambda _: tune_config,
      backend="ray",
      keep_checkpoints_num=1,
      n_trials=9,
      scheduler=scheduler,
      resources_per_trial={"cpu" : 2, "gpu" : 1},
      progress_reporter=reporter,
      stop={"training_iteration": 1},
      local_dir="./ray_results/",
      name="tune_transformer_pbt",
      log_to_file=True,
  )

# fighterList = ["cardiffnlp/twitter-roberta-base-sentiment-latest", #0
#                "finiteautomata/bertweet-base-sentiment-analysis",  #1
#                "ElKulako/cryptobert"                               #2
#                ]

"""## cardiffnlp/twitter-roberta-base-sentiment-latest"""

#Google Translate
fighter = fighterList[0]
dataPrep(0, fighter)

start()

tryTest("256GT")

#Google Translate Half
fighter = fighterList[0]
dataPrep(1, fighter)

start()

tryTest("256GTH")

#Transformer
fighter = fighterList[0]
dataPrep(2, fighter)

start()

tryTest("256T")

#Pure
fighter = fighterList[0]
dataPrep(2, fighter)

start()

tryTest("256Pure")

"""## Finiteautomata"""

#Google Translate Half
fighter = fighterList[1]
dataPrep(0, fighter)

start()

tryTest("128GT")

#Google Translate Half
fighter = fighterList[1]
dataPrep(1, fighter)

start()

tryTest("128GTH")

#Transformer
fighter = fighterList[1]
dataPrep(2, fighter)

start()

tryTest("128T")

#Transformer
fighter = fighterList[1]
dataPrep(2, fighter)

start()

tryTest("128Pure")

"""##ElKulako/cryptobert"""

#Google Translate Full
fighter = fighterList[2]
dataPrep(0, fighter)

start()

tryTest('128_2GT')

#Google Translate Half
fighter = fighterList[2]
dataPrep(1, fighter)

start()

tryTest("128_2GTH")

#Transformer
fighter = fighterList[2]
dataPrep(2, fighter)

start()

tryTest("128_2T")

#Transformer
fighter = fighterList[2]
dataPrep(2, fighter)

start()

tryTest("128_2Pure")

"""#Cardnif Again"""

#Google Translate Full
fighter = fighterList[3]
dataPrep(0, fighter)

start()

tryTest('256_2GT')

#Google Translate Half
fighter = fighterList[3]
dataPrep(1, fighter)

start()

tryTest('256_2GTH')

#Google Translate Half
fighter = fighterList[3]
dataPrep(2, fighter)

start()

tryTest('256_2T')

#Google Translate Half
fighter = fighterList[3]
dataPrep(2, fighter)

start()

tryTest('256_2Pure')

"""##Other Ray"""

import os
os.environ['TORCH_USE_CUDA_DSA'] = '1'

from transformers import (AutoModelForSequenceClassification, AutoTokenizer,
                          Trainer, TrainingArguments)

def model_init():
    return AutoModelForSequenceClassification.from_pretrained(
        fighter, return_dict=True)

# def compute_metrics(eval_pred):
#     predictions, labels = eval_pred
#     predictions = predictions.argmax(axis=-1)
#     return metric.compute(predictions=predictions, references=labels)

# Evaluate during training and a bit more often
# than the default to be able to prune bad trials early.
# Disabling tqdm is a matter of preference.
training_args = TrainingArguments(
    "test",
    evaluation_strategy="steps",
    eval_steps=500,
    num_train_epochs=3,
    per_device_eval_batch_size=32,
    warmup_steps=500,
    weight_decay=0.3,
    disable_tqdm=True)

trainer = Trainer(
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    model_init=model_init,
    compute_metrics=compute_metrics,
)

# Default objective is the sum of all metrics
# when metrics are provided, so we have to maximize it.
trainer.hyperparameter_search(
    direction="maximize",
    backend="ray",
    n_trials=4 # number of trials
)

"""# Optuna"""

!pip install optuna -q
!pip install git+https://github.com/PyTorchLightning/pytorch-lightning -q
import pytorch_lightning as pl

import numpy as np
import optuna
import torch
from optuna.integration import PyTorchLightningPruningCallback
from pytorch_lightning.callbacks import EarlyStopping
from sklearn.preprocessing import MaxAbsScaler
from torchmetrics import MeanAbsolutePercentageError

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from pytorch_lightning import LightningModule
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import Callback

class PyTorchLightningPruningCallback(Callback):
    """PyTorch Lightning callback to prune unpromising trials.
    See `the example <https://github.com/optuna/optuna-examples/blob/
    main/pytorch/pytorch_lightning_simple.py>`__
    if you want to add a pruning callback which observes accuracy.
    Args:
        trial:
            A :class:`~optuna.trial.Trial` corresponding to the current evaluation of the
            objective function.
        monitor:
            An evaluation metric for pruning, e.g., ``val_loss`` or
            ``val_acc``. The metrics are obtained from the returned dictionaries from e.g.
            ``pytorch_lightning.LightningModule.training_step`` or
            ``pytorch_lightning.LightningModule.validation_epoch_end`` and the names thus depend on
            how this dictionary is formatted.
    """

    def __init__(self, trial: optuna.trial.Trial, monitor: str) -> None:
        super().__init__()

        self._trial = trial
        self.monitor = monitor

    def on_validation_end(self, trainer: Trainer, pl_module: LightningModule) -> None:
        # When the trainer calls `on_validation_end` for sanity check,
        # do not call `trial.report` to avoid calling `trial.report` multiple times
        # at epoch 0. The related page is
        # https://github.com/PyTorchLightning/pytorch-lightning/issues/1391.
        if trainer.sanity_checking:
            return

        epoch = pl_module.current_epoch

        current_score = trainer.callback_metrics.get(self.monitor)
        if current_score is None:
            message = (
                "The metric '{}' is not in the evaluation logs for pruning. "
                "Please make sure you set the correct metric name.".format(self.monitor)
            )
            warnings.warn(message)
            return

        self._trial.report(current_score, step=epoch)
        if self._trial.should_prune():
            message = "Trial was pruned at epoch {}.".format(epoch)
            raise optuna.TrialPruned(message)

from transformers import Trainer
# define objective function
def objective(trial):
    global trainer
    # Other hyperparameters
    learning_rate = trial.suggest_float("learning_rate", 0.00001, 0.00005)
    warmup_steps = trial.suggest_categorical("warmup_steps", [0, 250, 500])
    weight_decay = trial.suggest_float("weight_decay", 0, 0.3)

    # throughout training we'll monitor the validation loss for both pruning and early stopping
    pruner = PyTorchLightningPruningCallback(trial, monitor="val_loss")
    early_stopper = EarlyStopping("val_loss", min_delta=0.001, patience=3, verbose=True)
    callbacks = [pruner, early_stopper]

    # reproducibility
    np.random.seed(42069)
    torch.manual_seed(42069)

    # build the Transformer model
    training_args = TrainingArguments(
        output_dir=f'./optuna',
        num_train_epochs = 5,
        per_device_train_batch_size = 16,
        per_device_eval_batch_size = 32,
        learning_rate = learning_rate,
        warmup_steps = int(warmup_steps),
        weight_decay = weight_decay,
        logging_dir='./logs',
        logging_steps=10,
        seed=69420,
        load_best_model_at_end=True,
        evaluation_strategy="steps",
        eval_steps=100,
        metric_for_best_model = 'f1',
        save_steps=100
    )

    # when validating during training, we can use a slightly longer validation
    model = AutoModelForSequenceClassification.from_pretrained(fighter)
    model.to(device)
    model.resize_token_embeddings(len(tokenizer))
    torch.cuda.empty_cache()

    trainer = Trainer(
        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=train_dataset,         # training dataset
        eval_dataset=val_dataset,            # evaluation dataset
        compute_metrics=compute_metrics,
        callbacks = callbacks
    )

    trainer.train()

    dfTest['sen_2'] = dfTest['text'].apply(testing)
    same_values_count = (dfTest['sen'] == dfTest['sen_2']).sum()
    Accuracy = same_values_count/300

    return Accuracy if Accuracy != np.nan else float("inf")

fighter = fighterList[2]
dataPrep(1, fighter)

# results_list = []

# for convenience, print some optimization trials information
def print_callback(study, trial):
    print(f"Current value: {trial.value}, Current params: {trial.params}")
    print(f"Best value: {study.best_value}, Best params: {study.best_trial.params}")

    # temp = trial.params
    # temp.update({"Accuracy" : trial.value,
    #              "eval_accuracy": trainer.evaluate()['eval_accuracy'],
    #              "eval_precision": trainer.evaluate()['eval_precision'],
    #              "eval_recall": trainer.evaluate()['eval_recall'],
    #              "eval_f1": trainer.evaluate()['eval_f1']
    #              })

    # results_list.append(temp)
    # dfResult = pd.DataFrame(results_list)
    # dfResult.to_csv("/content/drive/MyDrive/result_128_2EDA2.csv", index = False)

# optimize hyperparameters by minimizing the sMAPE on the validation set
if __name__ == "__main__":
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=20, callbacks=[print_callback])

fighter = fighterList[2]
dataPrep(5, fighter)

results_list = []

# for convenience, print some optimization trials information
def print_callback(study, trial):
    print(f"Current value: {trial.value}, Current params: {trial.params}")
    print(f"Best value: {study.best_value}, Best params: {study.best_trial.params}")

    # temp = trial.params
    # temp.update({"Accuracy" : trial.value})
    # results_list.append(temp)
    # dfResult = pd.DataFrame(results_list)
    # dfResult.to_csv("/content/drive/MyDrive/result_128_2EDA2.csv", index = False)

# optimize hyperparameters by minimizing the sMAPE on the validation set
if __name__ == "__main__":
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=20, callbacks=[print_callback])

fighter = fighterList[1]
dataPrep(0, fighter)

results_list = []

# for convenience, print some optimization trials information
def print_callback(study, trial):
    print(f"Current value: {trial.value}, Current params: {trial.params}")
    print(f"Best value: {study.best_value}, Best params: {study.best_trial.params}")

    temp = trial.params
    temp.update({"Accuracy" : trial.value})
    results_list.append(temp)
    dfResult = pd.DataFrame(results_list)
    dfResult.to_csv("/content/drive/MyDrive/result_128GT.csv", index = False)

# optimize hyperparameters by minimizing the sMAPE on the validation set
if __name__ == "__main__":
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=20, callbacks=[print_callback])

"""# Pushing Established Model to Huggingface"""

def sentiment_score128(review, model):
    # Tokenize the review outside the loop
    tokens = tokenizer(review, return_tensors='pt', max_length = 128, truncation = True).input_ids.to(device)

    # Pass the tokens directly to the model for batch processing
    result = model(tokens)

    # Convert the tensor to a numpy array and extract the predicted sentiment
    sentiment = int(torch.argmax(result.logits))

    return sentiment

def sentiment_score256(review, model):
    # Tokenize the review outside the loop
    tokens = tokenizer(review, return_tensors='pt', max_length = 256, truncation = True).input_ids.to(device)

    # Pass the tokens directly to the model for batch processing
    result = model(tokens)

    # Convert the tensor to a numpy array and extract the predicted sentiment
    sentiment = int(torch.argmax(result.logits))

    return sentiment

f = open("Skripsi/cryptoVocab.txt", "r")
crypto_vocabulary = f.read().split(',')
crypto_vocabulary = [term.replace('"', '') for term in crypto_vocabulary]

name = "results6"
shutil.unpack_archive(f"/content/drive/MyDrive/{name}.zip", f"./{name}")

from transformers import set_seed
seed = 69420

set_seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

name = "./results6/checkpoint-500"
from transformers import AutoTokenizer, RobertaForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("ElKulako/cryptobert")

try:
  new_tokens = set(crypto_vocabulary) - set(tokenizer.vocab.keys())
except:
  new_tokens = set(crypto_vocabulary) - set(tokenizer.get_vocab().keys())

tokenizer.add_tokens(list(new_tokens))

#-----
model = RobertaForSequenceClassification.from_pretrained(name)
device = 'cuda'
model.to(device)
print('Done')

dfTest['sen_2'] = dfTest['text'].apply(sentiment_score256, args = (model,))

accOvr = len(dfTest[(dfTest["sen_2"]) == dfTest['sen']]) / 300
accNeg = len(dfTest[(dfTest["sen_2"]==0) & (dfTest['sen']==0)]) / 100
accNeu = len(dfTest[(dfTest["sen_2"]== 1) & (dfTest['sen']== 1)]) / 100
accPos = len(dfTest[(dfTest["sen_2"]== 2) & (dfTest['sen']== 2)]) / 100

print(f"Overall-{accOvr}\nNegative-{accNeg}\nNeutral-{accNeu}\nPositive-{accPos}\n")

from huggingface_hub import notebook_login
notebook_login()

modelName = "cryptobertRefined"
trainer.push_to_hub(modelName)
tokenizer.push_to_hub(modelName)

"""#Top Model"""

DIR = "./Skripsi/gridSearchNLP/"

import os

# Get a list of all files in the directory
all_files = os.listdir(DIR)

# Filter out files that are not paramsSenFinale.csv
csv_files = [file for file in all_files if file.endswith('.csv') and file != 'paramsSenFinale.csv']

# Print the remaining files
print(csv_files)

import re

dfResult = pd.DataFrame()
for x in csv_files:
  temp = pd.read_csv(DIR+f"{x}", usecols = ['Accuracy'])
  modelName = x.split('.csv')[0]
  modelName = modelName.split('result_')[1]
  temp['modelName'] = modelName
  temp.set_index('modelName', inplace = True)
  temp.sort_values(by = 'Accuracy', ascending = False, inplace = True)
  temp = temp.iloc[[0]].copy()
  dfResult = pd.concat([dfResult, temp])

# Separate numbers and alphabets in the index
numbers = []
alphabets = []
for index_value in dfResult.index:
    # Use regular expression to separate numbers and alphabets
    parts = re.match(r'(\d+)_?(\d*)?([A-Za-z]+)', index_value).groups()
    numbers.append(f'{parts[0]}{"_" + parts[1] if parts[1] else ""}')
    alphabets.append(parts[2])

# Add new columns for numbers and alphabets
dfResult['model'] = numbers
dfResult['data'] = alphabets

dfResult.reset_index(inplace = True, drop = True)

# Assuming your DataFrame is named df
dfResult['model'] = dfResult['model'].map({
    '256': 'twitter-roberta-base-sentiment-latest',
    '256_2': 'twitter-xlm-roberta-base-sentiment',
    '128': 'bertweet-base-sentiment-analysis',
    '128_2': 'cryptobert'
})

dfResult = dfResult.pivot(index='model', columns='data', values='Accuracy')
dfResult.loc['mean'] = dfResult.mean()
dfResult['mean'] = dfResult.mean(axis=1)

# If there are other values in the 'model' column, they will be set to NaN in the 'new_model' column.
dfResult.head()