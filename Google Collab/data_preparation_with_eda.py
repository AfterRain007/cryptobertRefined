# -*- coding: utf-8 -*-
"""Data Preparation with EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18nNpPCMcSKgK5QsFUIVJ3737g1u7FfPJ


# Load Indenpenciesiseaus
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import re
!git clone https://github.com/AfterRain007/Skripsi
!ls
!pip install transformers -q
!pip install sentencepiece -q
import time
import shutil
# from google.colab import drive
# drive.mount('/content/drive')

"""# Data Preparation

## Manually Labelled Data
"""

DIR = "Skripsi/Trash/Manual/"

df = pd.read_csv(DIR + "manualLabelled.csv", usecols=['text', 'sen'])
df2 = pd.read_csv(DIR + "negSenVerified.csv", usecols=['text', 'sen'])
df3 = pd.read_csv(DIR + "negSenVerified2.csv", usecols=['text', 'sen'])
df4 = pd.read_csv(DIR + "negSenVerified3.csv", usecols=['text', 'sen'])
df5 = pd.read_csv(DIR + "negSenVerified4.csv", usecols=['text', 'sen'])
df6 = pd.read_csv(DIR+"Crypto Sentiment Dataset.csv", usecols=['Comment Text', 'Sentiment'])
df7 = pd.read_csv(DIR+"spreadsheetNegVer.csv", usecols=['text', 'sen'])

df2 = df2.loc[df2['sen'] == 1].assign(sen=-1)
df3 = df3.loc[df3['sen'] == 1].assign(sen=-1)
df4 = df4.loc[df4['sen'] == 1].assign(sen=-1)
df5 = df5.loc[df5['sen'] == 1].assign(sen=-1)

sentiment_mapping = {'Negative': -1, 'Neutral': 0, 'Positive': 1}
df6['Sentiment'] = df6['Sentiment'].map(sentiment_mapping)
df6 = df6.rename(columns={"Comment Text":"text", 'Sentiment':'sen'})

df = pd.concat([df, df2, df3, df4, df5, df6, df7])

df = df[df['sen']<2]
df.reset_index(inplace = True, drop = True)
print(f"Final amount of Fine Tune data: {len(df)}")
df['sen'].value_counts()

"""### Cleaning"""

#Import all the module
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import torch
from html import unescape

#Delete """" if you haven't download

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

#Delete """" if you haven't download

# Compile the regular expressions outside the clean_text function
url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
ftp_pattern = re.compile(r'ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')

# Set of punctuation characters
punctuation_set = set(string.punctuation)

def clean_text(text):
    t = re.sub(url_pattern, ' ', text)  # remove urls if any
    t = re.sub(ftp_pattern, ' ', t)  # remove urls if any
    t = unescape(t)  # html entities fix

    # Convert text to lowercase
    text = t.lower()

    # Remove punctuation
    text = ''.join(char for char in text if char not in punctuation_set)

    # Tokenization
    tokens = word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words("english"))
    tokens = [token for token in tokens if token not in stop_words]

    # Remove special characters and numbers
    tokens = [re.sub(r"[^a-zA-Z]", "", token) for token in tokens]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Remove unnecessary spaces
    tokens = [token.strip() for token in tokens if token.strip()]

    # Join tokens back into a single string
    cleaned_text = " ".join(tokens)

    return cleaned_text

# Example usage
text = "This is an example sentence! 123# https://www.youtube.com/watch?v=_9bw_VtMUGA"
cleaned_text = clean_text(text)
print(cleaned_text)

df['text'] = df['text'].astype(str)
df['text'] = df['text'].apply(clean_text)

df['WC'] = df['text'].apply(lambda x: len(x.split()))
print(len(df[df['WC']>4]))

df = df[~((df['WC'] < 4) & ((df['sen'] == 1) | (df['sen'] == 0)))].sort_values(by=['WC'])

len(df)

df = df[~((df['text'].str.contains('bitcoin going', flags=re.IGNORECASE)) &
      ~(df['text'].str.contains('bitcoin okay bro', flags=re.IGNORECASE)) &
      ~(df['text'].str.contains('explode', flags=re.IGNORECASE)) &
      ~(df['text'].str.contains('know going', flags=re.IGNORECASE)) &
      ~(df['text'].str.contains('bitcoin k loading', flags=re.IGNORECASE)) &
      ~(df['text'].str.contains('full audit usdc', flags=re.IGNORECASE)) &
      ~(df['text'].str.contains('record didnt need', flags=re.IGNORECASE)) &
      ~(df['text'].str.contains('squawkcnbc jerrymoran bitcoin', flags=re.IGNORECASE)) &
      ((df['sen'] != -1) | (df['WC'] > 4)))]

df.reset_index(inplace = True, drop = True)

del dfTrain['WC']
del dfTest['WC']
del dfVal['WC']

"""### Partitioninigninnigg"""

# Create a new DataFrame with 250 rows of each label
sample_size = 200

dfM1 = df[df['sen'] == -1].head(sample_size)
df0 = df[df['sen'] == 0].head(sample_size)
df1 = df[df['sen'] == 1].head(sample_size)

dfVT = pd.concat([dfM1, df0, df1])

indices_to_remove = dfVT.index

sample_size = 100
dfM1 = dfVT[dfVT['sen'] == -1].head(sample_size)
df0 = dfVT[dfVT['sen'] == 0].head(sample_size)
df1 = dfVT[dfVT['sen'] == 1].head(sample_size)

dfVal = pd.concat([dfM1, df0, df1])

dfM1 = dfVT[dfVT['sen'] == -1].tail(sample_size)
df0 = dfVT[dfVT['sen'] == 0].tail(sample_size)
df1 = dfVT[dfVT['sen'] == 1].tail(sample_size)

dfTest = pd.concat([dfM1, df0, df1])

dfTrain = df.drop(indices_to_remove)
dfTrain.reset_index(inplace = True, drop = True)
dfVal.reset_index(inplace = True, drop = True)
dfTest.reset_index(inplace = True, drop = True)

dfTrain.to_csv("dfTrain.csv", index = False)
dfVal.to_csv("dfVal.csv", index = False)
dfTest.to_csv("dfTest.csv", index = False)

print(f"Banyak data train: {len(dfTrain)}")
print(f"Banyak data validation: {len(dfVal)}")
print(f"Banyak data test: {len(dfTest)}")

"""## Sentiment Labelling

### Loading Raw Data
"""

!pip install opendatasets -q
!pip install kaggle -q

import opendatasets as od
import json

! cp ./Skripsi/kaggle.json ./
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

# Download datasets using opendatasets
od.download("https://www.kaggle.com/datasets/kaushiksuresh147/bitcoin-tweets")
od.download("https://www.kaggle.com/datasets/afterrain007/reddit-bitcoin-topic-text")

"""### Cleaning"""

import gc

# reading the CSV file
df = pd.read_csv('./bitcoin-tweets/Bitcoin_tweets.csv', usecols = ['date', 'text'], dtype = {"date": str, "text" : str})
df['date'] = pd.to_datetime(df['date'], errors='coerce')
df.dropna(inplace = True)
gc.collect()
df.head()

dfReddit = pd.read_csv('./reddit-bitcoin-topic-text/redditText.csv', usecols = ['timestamp', 'comments'])
dfReddit.rename(columns={'comments': 'text', 'timestamp' : 'date'}, inplace=True)
dfReddit['date'] = pd.to_datetime(dfReddit['date'], unit='s', errors='coerce')
dfReddit = dfReddit[dfReddit['text'] != '[deleted]']
dfReddit.dropna(inplace = True)

gc.collect()

df = pd.concat([df, dfReddit])
df.set_index('date', inplace=True)
print(len(df))

df = df.sort_index()

# df['text'] = df['text'].astype(str)
# #Spam Tweet Example

# dfSpam1 = df[df['text'].str.contains('Compared to the last tweet, the price has', flags=re.IGNORECASE)].head()
# dfSpam2 = df[df['text'].str.contains('Bitcoin Whale Alert:', flags=re.IGNORECASE)].head()
# dfSpam3 = df[df['text'].str.contains('#bitcoin LONGED', flags=re.IGNORECASE)].head()
# dfSpam4 = df[df['text'].str.contains('Market Cap. Swap', flags=re.IGNORECASE)].head()
# dfSpam5 = df[df['text'].str.contains('Bitcoin BTC Current Price:', flags=re.IGNORECASE)].head()

# dfSpam = pd.concat([dfSpam1[:1],dfSpam2[:1],dfSpam3[:1],dfSpam4[:1],dfSpam5[:1]])
# dfSpam.to_excel("dfSpam.xlsx")
# len(dfSpam)

print(len(df))

df = df[(~df['text'].str.contains('Compared to the last tweet, the price has', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('In the last 24 hours the price has', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('transferred from Unknown Wallet to Unknown Wallet', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Market Cap. Swap', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Bitcoin BTC Current Price:', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Current #Bitcoin Price is', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Bitcoin Whale Alert:', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('tx with a value of', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('will be transfered from', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Someone just transfered', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('is a super underrated bitcoiner I’ve been following', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('has been transfered to an', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('transferred from unknown wallet to', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Someone just transfered', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('is a super underrated bitcoiner I’ve been following her tweets and tips', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Market Cap. Swap', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Bitcoin BTC Current Price:', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Current #Bitcoin Price is', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Bitcoin Whale Alert:', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('will be transfered from', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('#bitcoin SHORTED', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('tx with a value of', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('#bitcoin LONGED', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('move from unknown wallet to', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('BTC - short alert', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('1 BTC Price: Bitstamp', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Trending Crypto Alert!', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('#Bitcoin mempool Tx summary in the last 60 seconds', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('BEARWHALE! just SHORTED', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Buyer alert:', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('#Bitcoin Price:', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Based on #coindesk BPI #bitcoin', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('BTCUSDT LONGED', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Everywhere should follow @', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('BULLWHALE! just LONGED', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Long Position Taken On $', flags=re.IGNORECASE)) &
        (~df['text'].str.contains("A new block was found on the #Bitcoin network. We're at block height", flags=re.IGNORECASE)) &
        (~df['text'].str.contains('current #bitcoin price is', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('today and watch your life turn around, start earning', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Symbol: BTCUSD (Binance)', flags=re.IGNORECASE, regex=False)) &
        (~df['text'].str.contains('Current  #Bitcoin Price:', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('transferred from #Coinbase to unknown wallet', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('SCAN RESULTS - 15m - #BTC PAIR', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('$BTC Latest Block Info: Block', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Scan results - #Gateio - 15m', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('BTCUSD LONGED @', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Follow for recent Bitcoin price updates.', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('1 BTC Price: Bitstamp', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('The current price of bitcoin is $', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Symbol:|Signal:|Price:|Volume:', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Deal Close:|Entry:|Entry Price:', flags=re.IGNORECASE)) &
        (~df['text'].str.contains('Scan results - | - 15m', flags=re.IGNORECASE))]

gc.collect()
print(len(df))

#Import all the module
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import torch
from html import unescape

#Delete """" if you haven't download

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

#Delete """" if you haven't download

# Compile the regular expressions outside the clean_text function
url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
ftp_pattern = re.compile(r'ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')

# Set of punctuation characters
punctuation_set = set(string.punctuation)

def clean_text(text):
    t = re.sub(url_pattern, ' ', text)  # remove urls if any
    t = re.sub(ftp_pattern, ' ', t)  # remove urls if any
    t = unescape(t)  # html entities fix

    # Convert text to lowercase
    text = t.lower()

    # Remove punctuation
    text = ''.join(char for char in text if char not in punctuation_set)

    # Tokenization
    tokens = word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words("english"))
    tokens = [token for token in tokens if token not in stop_words]

    # Remove special characters and numbers
    tokens = [re.sub(r"[^a-zA-Z]", "", token) for token in tokens]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Remove unnecessary spaces
    tokens = [token.strip() for token in tokens if token.strip()]

    # Join tokens back into a single string
    cleaned_text = " ".join(tokens)

    return cleaned_text

# Example usage
text = "This is an example sentence! 123# https://www.youtube.com/watch?v=_9bw_VtMUGA"
cleaned_text = clean_text(text)
print(cleaned_text)

df['text'] = df['text'].apply(clean_text)

df['WC'] = df['text'].apply(lambda x: len(x.split()))
df = df[df['WC']>=4]

del df['WC']
gc.collect()
df.head()

df['time'] = df.index
df.drop_duplicates(inplace = True)
gc.collect()
del df['time']

df.to_csv("dfText.csv")
shutil.copy(f'dfText.csv', '/content/drive/My Drive/')

"""### Labelling"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

df = pd.read_csv("/content/drive/My Drive/dfText.csv", usecols = ['date', 'text'])
df['date'] = pd.to_datetime(df['date'],errors='coerce')
df.set_index('date', inplace = True)

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import concurrent
import time as time
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

seed = 69420
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

modelName = "AfterRain007/cryptobertRefined"
tokenizer = AutoTokenizer.from_pretrained(modelName)
model = AutoModelForSequenceClassification.from_pretrained(modelName)

model.to(device)
torch.cuda.empty_cache()

def sentiment_score(review):
    # Tokenize the review outside the loop
    tokens = tokenizer(review, return_tensors='pt', max_length = 128, truncation = True).input_ids.to(device)

    # Pass the tokens directly to the model for batch processing
    result = model(tokens)

    # Convert the tensor to a numpy array and extract the predicted sentiment
    sentiment = int(torch.argmax(result.logits)) - 1

    return sentiment

len(df)

dfTemp = df[4800000:]
dfTemp['sen'] = dfTemp['text'].apply(sentiment_score)

dfTemp

dfTemp.to_csv(f'dfTextSenLast.csv')
shutil.copy(f'dfTextSenLast.csv', "/content/drive/My Drive/")

from transformers import set_seed
import os
seed = 69420
set_seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

temp = pd.DataFrame()
chunk_size = 100000
skip = 22

for x in np.arange(int(len(df)/chunk_size)):
  if x <= skip:
    continue
  print(f"Starting iteration-{x}")
  dfTemp = df[x*chunk_size:(x+1)*chunk_size]
  dfTemp['sen'] = dfTemp['text'].apply(sentiment_score)
  temp = pd.concat([temp, dfTemp])
  temp.to_csv(f'dfTextSen2_2-{x}.csv')
  shutil.copy(f'dfTextSen2_2-{x}.csv', "/content/drive/My Drive/")
  if os.path.exists(f"/content/drive/My Drive/dfTextSen2_2-{x-1}.csv"):
    os.remove(f"/content/drive/My Drive/dfTextSen2_2-{x-1}.csv")

"""#EDA

## Loading Data

### Function Pool
"""

#Import all the module
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import torch
from html import unescape

#Delete """" if you haven't download

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

#Delete """" if you haven't download

# Compile the regular expressions outside the clean_text function
url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
ftp_pattern = re.compile(r'ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')

# Set of punctuation characters
punctuation_set = set(string.punctuation)

def clean_text(text):
    t = re.sub(url_pattern, ' ', text)  # remove urls if any
    t = re.sub(ftp_pattern, ' ', t)  # remove urls if any
    t = unescape(t)  # html entities fix

    # Convert text to lowercase
    text = t.lower()

    # Remove punctuation
    text = ''.join(char for char in text if char not in punctuation_set)

    # Tokenization
    tokens = word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words("english"))
    tokens = [token for token in tokens if token not in stop_words]

    # Remove special characters and numbers
    tokens = [re.sub(r"[^a-zA-Z]", "", token) for token in tokens]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Remove unnecessary spaces
    tokens = [token.strip() for token in tokens if token.strip()]

    # Join tokens back into a single string
    cleaned_text = " ".join(tokens)

    return cleaned_text

# Example usage
text = "This is an example sentence! 123# https://www.youtube.com/watch?v=_9bw_VtMUGA"
cleaned_text = clean_text(text)
print(cleaned_text)

def has_repeating_word(text, repetition_threshold=3):
    # Find all words in the text
    words = re.findall(r'\b\w+\b', text.lower())

    # Create a regular expression pattern for detecting repeating words
    pattern = r'\b(\w+)' + r'(\s+\1){%d,}\b' % (repetition_threshold - 1)

    # Search for the pattern in the text
    match = re.search(pattern, ' '.join(words))

    # Return True if a match is found, indicating repeating words
    return match is not None

def preprocess_dataframe(df, spammy):
  df['text'] = df['text'].astype(str)
  df['text'] = df['text'].apply(clean_text)
  df['WC']   = df['text'].apply(lambda x: len(x.split()))
  df['spam'] = df['text'].apply(has_repeating_word, args=(spammy,))
  df = df[(df['WC'] >= 4) & (df['spam'] == False)].copy()
  df.drop(['WC', 'spam'], axis = 1, inplace = True)
  df = pd.concat([dfTrain, df])
  df.drop_duplicates(subset=['text'], keep=False).reset_index(drop = True, inplace = True)
  return df

def preprocess_dataframe2(df):
  minLen = len(df[df['sen'] == -1])
  df = df[df['sen'].isin([-1, 0, 1])].groupby('sen').head(minLen)
  df.reset_index(inplace = True, drop = True)
  return df

from sklearn.preprocessing import MinMaxScaler
def transform(year, year2=0):
  scaler = MinMaxScaler()
  if year2 == 0:
    temp  = df.loc[year]
    temp1 = scaler.fit_transform(temp)
    temp1 = pd.DataFrame(temp1,columns=temp.columns, index=temp.index)
    return temp1
  temp  = df.loc[year:year2]
  temp1 = scaler.fit_transform(temp)
  temp1 = pd.DataFrame(temp1,columns=temp.columns, index=temp.index)
  return temp1

"""### Sen"""

!pip install opendatasets -q
!pip install kaggle -q

import opendatasets as od
import json
import gc

! cp ./Skripsi/Trash/kaggle.json ./
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

#Download datasets using opendatasets
od.download("https://www.kaggle.com/datasets/afterrain007/bitcoin-tweet-and-reddit-labelled")

from sklearn.preprocessing import MinMaxScaler
# reading the CSV file
dfSen = pd.read_csv('./bitcoin-tweet-and-reddit-labelled/Text.csv', usecols = ['date', 'sen'], dtype = {"date": str,  'sen' : int})
dfSen['date'] = pd.to_datetime(dfSen['date'], errors='coerce')

dfSen['date'] = dfSen['date'].dt.date
dfSen.set_index('date', inplace = True)

gc.collect()
dfSen.head()

"""### Training Data"""

dfTrain = pd.read_csv("./Skripsi/manualCleanAF/dfTrain.csv")

#Google Translate Data

DIR = "Skripsi/AugmentationGT/"

frlng = ['it', 'fr', "sv", "da", 'pt', 'id', 'pl', 'hr', "bg", "fi",
         "no", 'ru', 'es', 'nl', 'af', 'de', "sk", "cs", 'lv', "sq"]
        #  'ko', 'zh', 'tl' 'gl', 'el', 'ar', 'hi', 'ja']

dfGt = pd.DataFrame()
for x in frlng:
  temp = pd.read_csv(DIR + f"dfTrain-{x}GT.csv")
  dfGt = pd.concat([dfGt, temp])
dfGt.reset_index(inplace = True, drop =  True)

dfGtH = dfGt[:int(len(dfGt)/2)].copy()

#Transformer Data

frlng2 = ['zh', 'es', 'ru', 'jap', 'de', 'fr', 'it', 'id']

DIR = "Skripsi/AugmentationT/"

dfT = pd.DataFrame()
for x in frlng2:
  temp = pd.read_csv(DIR + f"dfTrain-{x}T.csv")
  dfT = pd.concat([dfT, temp])

dfT.reset_index(inplace = True, drop =  True)

dfT = dfT[~dfT['text'].str.contains("- I'm sorry. - I'm sorry.") &
          ~dfT['text'].str.contains("Runtime 512 Error here baby!") &
          ~dfT['text'].str.contains("The Bitcoin Treasure is worth our bitcoin.")]

dfT.reset_index(inplace = True, drop = True)

print(dfGt.head(2))
print(dfGtH.tail(2))
print(dfT.head(2))

dfTest = pd.read_csv("./Skripsi/manualCleanAF/dfTest.csv", usecols = ['text', 'sen'])
dfVal = pd.read_csv("./Skripsi/manualCleanAF/dfVal.csv", usecols = ['text', 'sen'])

dfTest

"""### Price & Volume"""

!pip install pycoingecko -q

# startTime = datetime.fromisoformat("2017-12-03")
# endTime = datetime.fromisoformat("2023-06-29")

# startTime = startTime.timestamp()
# endTime = endTime.timestamp()

# from pycoingecko import CoinGeckoAPI
# cg = CoinGeckoAPI()

# bitcoin = cg.get_coin_market_chart_range_by_id(id='bitcoin',vs_currency='usd',from_timestamp=startTime,to_timestamp=endTime)

# prices = [data[1] for data in bitcoin['prices']]
# market_caps = [data[1] for data in bitcoin['market_caps']]
# volumes = [data[1] for data in bitcoin['total_volumes']]

import pycoingecko
import matplotlib.pyplot as plt
import datetime

# Initialize CoinGecko API client
coinGecko = pycoingecko.CoinGeckoAPI()
# Get historical price data for Bitcoin
btc_data = coinGecko.get_coin_market_chart_by_id('bitcoin', 'usd', '5000days')
# Extract the dates and prices from the data
dates = [data[0] for data in btc_data['prices']]
# convert unix timestamp to datetime
dates = [
    datetime.datetime.fromtimestamp(date/1000)
    for date in dates
]
prices = [data[1] for data in btc_data['prices']]
volumes = [data[1] for data in btc_data['total_volumes']]

import pycoingecko
import matplotlib.pyplot as plt
import datetime
import pandas as pd

# Initialize CoinGecko API client
coinGecko = pycoingecko.CoinGeckoAPI()

# Cryptocurrencies to fetch
cryptos = ['bitcoin', 'ethereum', 'solana', 'ripple', 'cardano', 'dogecoin']

# Function to get historical price data for a cryptocurrency
def get_crypto_data(crypto):
    data = coinGecko.get_coin_market_chart_by_id(crypto, 'usd', '5000days')
    dates = [data[0] for data in data['prices']]
    dates = [datetime.datetime.fromtimestamp(date/1000) for date in dates]
    prices = [price[1] for price in data['prices']]
    return pd.DataFrame({'Date': dates, f'{crypto}': prices})

# Create a DataFrame for each cryptocurrency
dfs = [get_crypto_data(crypto) for crypto in cryptos]

# Merge DataFrames on the 'Date' column
crypto_df = pd.merge(dfs[0], dfs[1], on='Date', how='outer')
for df in dfs[2:]:
    crypto_df = pd.merge(crypto_df, df, on='Date', how='outer')

# Set 'Date' column as index
crypto_df.set_index('Date', inplace=True)

# Display the DataFrame
crypto_df.head()

df = crypto_df.copy()
df.dropna(inplace = True)

from matplotlib.dates import YearLocator

# Assuming df has a datetime index, if not, convert it to datetime index
# df.index = pd.to_datetime(df.index)

plt.figure(figsize=(10, 6))

cryptos = ['bitcoin', 'ethereum', 'solana', 'ripple', 'cardano', 'dogecoin']
for x in cryptos:
  plt.plot(df.index, df[x], label=x)

# Set x-axis ticks at yearly intervals
plt.gca().xaxis.set_major_locator(YearLocator())
plt.legend(loc='upper right', ncol=2, fontsize='small')
plt.grid()
plt.show()

df = pd.DataFrame({'date': dates, f'price': prices})
df['date'] = pd.to_datetime(df['date'])
df['date'] = df['date'].dt.date
df.set_index('date', inplace = True)

df['PC'] = df['price'].pct_change()

(df['PC'] < -0.1).sum()

df.sort_values(by = "PC")

scaler = MinMaxScaler()
scaler.fit(df[['bitcoin']])

for x in cryptos:
  df[[x]] = scaler.transform(df[[x]])

dfPrice = pd.DataFrame({
    'price': prices,
    'volume': volumes,
    'market_cap': market_caps},
     index=pd.to_datetime(dates))

dfPrice.index = dfPrice.index.date
dfPrice.index = pd.to_datetime(dfPrice.index)
dfPrice.index.name = 'date'

dfPrice['change'] = dfPrice['price'].pct_change() * 100
dfPrice.sort_values(by = 'change').head()

dfPrice.loc["2020-03-12": "2020-03-14"]

dfPrice.head(1)

plt.figure(figsize=(10, 6))  # Adjust the values in figsize to make the plot wider or narrower
plt.plot(dfPrice['price'])
plt.grid()
plt.title('Bitcoin Price Through The Year (in USD)')
plt.ylabel('Price')
plt.xlabel('Date')
plt.show()

df = pd.read_csv("multiTimeline.csv")
input = df['Category: All categories'].iloc[1:]
df2 = pd.DataFrame()
df2['trend'] = input
df2['date'] = pd.date_range(start='2016-01', end='2024-02', freq='M')
df2.set_index('date', inplace = True)
# df2.head()
df2.sort_index(inplace=True)
df2['trend'] = df2['trend'].astype(int)

plt.figure(figsize=(10, 6))  # Adjust the values in figsize to make the plot wider or narrower
plt.plot(df2.loc['2018-12-31':]['trend'])
plt.grid()
plt.title('Bitcoin Trend Through The Year (By Google Trends)')
plt.ylabel('Trend')
plt.xlabel('Date')
plt.show()

"""### Trends"""

tipes = {"Day": str,
         "bitcoin: (Worldwide)" : int}

dfTrends = pd.read_csv(f"./Skripsi/Trends/multiTimeline ({0}).csv", dtype = tipes, skiprows=1)
for x in np.arange(7):
  temp = pd.read_csv(f"./Skripsi/Trends/multiTimeline ({x+1}).csv", dtype = tipes, skiprows=1)
  # print(temp.head(1))
  temp2 = pd.merge(dfTrends, temp, on='Day', how='inner')
  # print(len(temp2))
  tempVar = temp2.iloc[0][1] / temp2.iloc[0][2]
  temp['bitcoin: (Worldwide)'] = temp['bitcoin: (Worldwide)'].apply(lambda x: x * tempVar)
  dfTrends = pd.concat([temp[:269], dfTrends])

dfTrends.rename(columns = {"bitcoin: (Worldwide)" : "trend", "Day" : "date"}, inplace = True)
dfTrends['date'] = pd.to_datetime(dfTrends['date'])
dfTrends.drop_duplicates(inplace = True)
dfTrends.set_index("date", inplace = True)
# dfTrends

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 100))

# Fit and transform the data
dfTrends['trend'] = scaler.fit_transform(dfTrends)

"""### Combine Them All!"""

dfSenCount = dfSen.groupby('date').mean()
scaler = MinMaxScaler(feature_range=(-1, 1))
dfSenCount['sen'] = scaler.fit_transform(dfSenCount)
dfPrice = dfPrice[dfTrends.index.min() : dfTrends.index.max()]

# Create a date range covering the entire period
start_date = dfSenCount.index.min()
end_date = dfSenCount.index.max()
date_range = pd.date_range(start_date, end_date)

# Reindex the DataFrame to include missing dates
dfSenCount = dfSenCount.reindex(date_range)

# Fill missing values with the mean of the surrounding data
dfSenCount.interpolate(method = 'linear', inplace = True)
# dfSenCount['sen'] = dfSenCount['sen'].fillna((dfSenCount['sen'].shift() + dfSenCount['sen'].shift(-1)) / 2)

# Create a date range covering the entire period
start_date = dfTrends.index.min()
end_date = dfTrends.index.max()
date_range = pd.date_range(start_date, end_date)

# Identify missing dates
print(date_range[~date_range.isin(dfPrice.index)])
print(date_range[~date_range.isin(dfSenCount.index)])
print(date_range[~date_range.isin(dfTrends.index)])

df = pd.concat([dfPrice, dfSenCount, dfTrends], axis = 1)
df.drop(['change', 'market_cap'], axis = 1, inplace = True)
df.head()

df.index.name = 'date'

df.head()

df.to_csv("dataRaw.csv")

"""## Sentiment Analysis

"""

dfGt = preprocess_dataframe(dfGt, 3)
dfGtH = preprocess_dataframe(dfGtH, 3)
dfT = preprocess_dataframe(dfT, 5)

import matplotlib.pyplot as plt

dfList = (df, dfGt, dfGtH, dfT)
dfName = ['Murni',
          'Google Translate20',
          'Google Translate10',
          'HelsinkiNLP']

fig, ax = plt.subplots(2, 2, figsize=(10, 8))

for i, x in enumerate(dfList):
    number_counts = x['sen'].value_counts()
    bars = ax[i // 2, i % 2].bar(number_counts.index, number_counts.values)

    for bar in bars:
        yval = bar.get_height()
        ax[i // 2, i % 2].text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom')

    ax[i // 2, i % 2].xaxis.set_ticks([-1, 0, 1])
    ax[i // 2, i % 2].set_title(dfName[i])

fig.suptitle("Jumlah Data Training")
plt.tight_layout()
plt.show()

dfTrain = preprocess_dataframe2(dfTrain)
dfGt = preprocess_dataframe2(dfGt)
dfGtH = preprocess_dataframe2(dfGtH)
dfT = preprocess_dataframe2(dfT)

import matplotlib.pyplot as plt

dfList = (dfTrain, dfGt, dfGtH, dfT)
dfName = ["Murni",
          'Google Translate20',
          'Google Translate10',
          'HelsinkiNLP']

fig, ax = plt.subplots(2, 2, figsize=(10, 8))

for i, x in enumerate(dfList):
    number_counts = x['sen'].value_counts()
    bars = ax[i // 2, i % 2].bar(number_counts.index, number_counts.values)

    for bar in bars:
        yval = bar.get_height()
        ax[i // 2, i % 2].text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom')

    ax[i // 2, i % 2].xaxis.set_ticks([-1, 0, 1])
    ax[i // 2, i % 2].set_title(dfName[i])

fig.suptitle("Jumlah Data Training")
plt.tight_layout()
plt.show()

number_counts = dfSen['sen'].value_counts()
bars = plt.bar(number_counts.index, number_counts.values)

plt.bar_label(bars, fmt='%d')

plt.xticks([-1, 0, 1])
plt.title("Total Sen Count")

plt.tight_layout()
plt.show()
print(f"\n{number_counts}")

"""### Sentiment Model"""

!pip install transformers -q
import torch
import random
from transformers import AutoTokenizer, RobertaForSequenceClassification

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

seed = 69420
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

f = open("Skripsi/cryptoVocab.txt", "r")
crypto_vocabulary = f.read().split(',')
crypto_vocabulary = [term.replace('"', '') for term in crypto_vocabulary]

def sentiment_score(review, length):
    # Tokenize the review outside the loop
    tokens = tokenizer(review, return_tensors='pt', max_length = length, truncation = True).input_ids.to(device)

    # Pass the tokens directly to the model for batch processing
    result = model(tokens)

    # Convert the tensor to a numpy array and extract the predicted sentiment
    sentiment = int(torch.argmax(result.logits)) - 1

    return sentiment

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import concurrent
import time as time
import random
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

modelList3 = ['cardiffnlp/twitter-roberta-base-sentiment-latest',  #512
              'finiteautomata/bertweet-base-sentiment-analysis',   #128
              'ProsusAI/finbert',                                  #512
              'cardiffnlp/twitter-xlm-roberta-base-sentiment',     #512
              'ElKulako/cryptobert',                               #512
              'svalabs/twitter-xlm-roberta-bitcoin-sentiment',     #512
              'AfterRain007/cryptobertRefined']                    #128

seed = 69420
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

time3 = []
dfFull = pd.DataFrame()

for x in modelList3[:6]:
    # Create a ThreadPoolExecutor with desired number of workers
    ## To make sentiment analysis faster
    ### Gotta go fast!

    tokenizer = AutoTokenizer.from_pretrained(x)
    try:
      new_tokens = set(crypto_vocabulary) - set(tokenizer.vocab.keys())
    except:
      new_tokens = set(crypto_vocabulary) - set(tokenizer.get_vocab().keys())
    tokenizer.add_tokens(list(new_tokens))

    model = AutoModelForSequenceClassification.from_pretrained(x)
    model.to(device)
    model.resize_token_embeddings(len(tokenizer))
    torch.cuda.empty_cache()

    start = time.time()

    if (x == 'finiteautomata/bertweet-base-sentiment-analysis'):
      sentiment_result = dfTest['text'].apply(sentiment_score, args=(128, ))
    else:
      sentiment_result = dfTest['text'].apply(sentiment_score, args=(512, ))

    dfFull[x.split('/')[-1]] = sentiment_result

modelName = "AfterRain007/cryptobertRefined"
tokenizer = AutoTokenizer.from_pretrained(modelName)
model     = RobertaForSequenceClassification.from_pretrained(modelName)
model.to(device)
torch.cuda.empty_cache()
dfFull[modelName.split('/')[-1]] = dfTest['text'].apply(sentiment_score, args=(128,))

dfFull['sen'] = dfTest['sen'].copy()
# Define sentiment values
sentiments = [-1, 0, 1]

fig, ax = plt.subplots(2, 3, figsize=(14, 8))

# Define the column names
columns = ["Model", "Acc Negative Sentiment", "Acc Neutral Sentiment", "Acc Positive Sentiment", "Overall Accuracy", "Time Cost"]

# Create an empty DataFrame
accDf = pd.DataFrame(columns=columns)

y_test = dfTest['sen']
for i, x in enumerate(modelList3[:6]):
    y_pred = dfFull[x.split('/')[-1]]
    cm = confusion_matrix(y_pred, y_test)

    # Create a heatmap
    sns.heatmap(cm, annot=True, fmt="d", cmap="YlGnBu", xticklabels=[-1, 0, 1], yticklabels=[-1, 0, 1], ax=ax[i // 3, i % 3])
    ax[i // 3, i % 3].set_title(x.split('/')[-1])
    ax[i // 3, i % 3].set_xlabel("Actual Sentiment")
    ax[i // 3, i % 3].set_ylabel("Predicted Sentiment")

plt.suptitle("All Model Confusion Matrix", fontsize = 13)
plt.tight_layout()
# plt.subplots_adjust(hspace=0.25)
plt.show()

dfFull['sen'] = dfTest['sen'].copy()

# Define the column names
columns = ["Model", "Acc Negative Sentiment", "Acc Neutral Sentiment", "Acc Positive Sentiment", "Overall Accuracy"]

# Create an empty DataFrame
accDf = pd.DataFrame(columns=columns)

for i, x in enumerate(modelList3):
  modelName = x.split('/')[-1]
  accOvr = len(dfFull[(dfFull[modelName]) == dfFull['sen']]) / 300
  accNeg = len(dfFull[(dfFull[modelName]==-1) & (dfFull['sen']==-1)]) / 100
  accNeu = len(dfFull[(dfFull[modelName]== 0) & (dfFull['sen']== 0)]) / 100
  accPos = len(dfFull[(dfFull[modelName]== 1) & (dfFull['sen']== 1)]) / 100
  rowData = [modelName] + [accNeg] + [accNeu] + [accPos] + [accOvr]
  accDf.loc[i] = rowData

accDf.to_excel("accuracyOverall.xlsx", index = False)
accDf

from sklearn.metrics import classification_report, accuracy_score

# Calculate accuracy
accuracy = accuracy_score(dfFull['sen'], dfFull[modelName])

# Generate a classification report
classification_report_result = classification_report(dfFull['sen'], dfFull[modelName], target_names=["-1", "0", "1"])

# Print the results
print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(classification_report_result)

y_test = dfTest['sen']
y_pred = dfFull[modelName]

# Create a confusion matrix
cm = confusion_matrix(y_pred, y_test)

# Plot the confusion matrix using seaborn and matplotlib
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="YlGnBu", xticklabels=[-1, 0, 1], yticklabels=[-1, 0, 1])
plt.xlabel('Actual Sentiment')
plt.ylabel('Predicted Sentiment')
plt.title('Confusion Matrix')
plt.show()

"""## Time Series

### Surface
"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
dfScaled = transform(None, None)
print("\n", dfScaled.head())

fig, ax = plt.subplots(2,2, figsize=(15, 8))

for i, x in enumerate(df.columns):
  ax[i%2, (i // 2) % 2].plot(df.index, df[x])
  ax[i%2, (i // 2) % 2].set_title(x.capitalize())

fig.suptitle('Line Plot of All Variable', fontsize=16)

plt.tight_layout()
plt.show()

fig, ax = plt.subplots(2,2, figsize=(15, 8))

for i, x in enumerate(df.columns):
  ax[i%2, (i // 2) % 2].plot(df.index, df[x])
  ax[i%2, (i // 2) % 2].set_title(x.capitalize())

fig.suptitle('Line Plot of All Variable')

plt.tight_layout()
plt.show()

dfScaled = transform(None, None)
#Variable Vs. Variable
fig, ax = plt.subplots(3, 1, figsize=(13, 10))

for i, x in enumerate(['volume', 'trend', 'sen']):
  ax[i].plot(dfScaled.index, dfScaled[x], label=x.capitalize(), color='blue')
  ax[i].plot(dfScaled.index, dfScaled['price'], label='Price',  color='red')
  ax[i].set(xlabel='Date', ylabel='Scaled Value', title=f'Price and {x.capitalize()}')
  ax[i].legend(), ax[i].grid(True)
  ax[i].set_title(f'Price Vs. {x.capitalize()}')

plt.tight_layout()
plt.show()

#Correlation Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(df.corr(), annot=True, cmap="YlGnBu", fmt=".2f", xticklabels=df.corr().index, yticklabels=df.corr().index)
plt.title("Correlation Matrix")

plt.tight_layout()
plt.show()

#Lag Plot
fig, ax = plt.subplots(1, 3, figsize=(13, 7))

for i, x in enumerate(['volume', 'trend', 'sen']):
  pd.plotting.lag_plot(df[['price', x]], lag=1, ax=ax[i])
  ax[i].set_title(f'1 Lag Plot: price vs. {x}')

fig.suptitle('Lag Plot')
plt.tight_layout()
plt.show()

from statsmodels.tsa.seasonal import MSTL
stl_kwargs = {"seasonal_deg": 0}

model = MSTL(df['price'], periods=(7, 365), stl_kwargs=stl_kwargs)
res = model.fit()

res.plot()
plt.tight_layout()
plt.show()

dfTemp = df.loc['2018':'2022'].copy()
from statsmodels.tsa.seasonal import MSTL
stl_kwargs = {"seasonal_deg": 0}
model = MSTL(dfTemp['price'], periods=(365), stl_kwargs=stl_kwargs)
res = model.fit()
res.plot()
plt.tight_layout()
plt.show()

"""### Detailed"""

import matplotlib.pyplot as plt
from matplotlib.dates import MonthLocator

# Create a figure with two subplots side by side
fig, ax = plt.subplots(2, 3, figsize=(15, 7))

for i, x in enumerate(df.index.year.unique()[1:]):
    temp = transform(str(x), 0)
    ax[i // 3, i % 3].plot(temp.index, temp['volume'], label='Volume', color='blue')
    ax[i // 3, i % 3].plot(temp.index, temp['price'], label='Price', color='red')

    ax[i // 3, i % 3].set(xlabel='Date', ylabel='Scaled Value', title=str(x))
    ax[i // 3, i % 3].xaxis.set_major_locator(MonthLocator(bymonth=[1, 4, 7, 10]))
    ax[i // 3, i % 3].grid(True)

# Adjust spacing between subplots
plt.suptitle("Price and Volume")
plt.tight_layout()

# Show the plots
plt.show()

import matplotlib.pyplot as plt
from matplotlib.dates import MonthLocator

# Create a figure with two subplots side by side
fig, ax = plt.subplots(2, 3, figsize=(15, 7))

for i, x in enumerate(df.index.year.unique()[1:]):
    temp = transform(str(x), 0)
    ax[i // 3, i % 3].plot(temp.index, temp['sen'], label='Sentiment', color='blue')
    ax[i // 3, i % 3].plot(temp.index, temp['price'], label='Price', color='red')

    ax[i // 3, i % 3].set(xlabel='Date', ylabel='Scaled Value', title=str(x))
    ax[i // 3, i % 3].xaxis.set_major_locator(MonthLocator(bymonth=[1, 4, 7, 10]))
    ax[i // 3, i % 3].grid(True)

# Adjust spacing between subplots
plt.suptitle("Price and Sentiment")
plt.tight_layout()

# Show the plots
plt.show()

import matplotlib.pyplot as plt
from matplotlib.dates import MonthLocator

# Create a figure with two subplots side by side
fig, ax = plt.subplots(2, 3, figsize=(15, 7))

for i, x in enumerate(df.index.year.unique()[1:]):
    temp = transform(str(x), 0)
    ax[i // 3, i % 3].plot(temp.index, temp['trend'], label='Trend', color='blue')
    ax[i // 3, i % 3].plot(temp.index, temp['price'], label='Price', color='red')

    ax[i // 3, i % 3].set(xlabel='Date', ylabel='Scaled Value', title=str(x))
    ax[i // 3, i % 3].xaxis.set_major_locator(MonthLocator(bymonth=[1, 4, 7, 10]))
    ax[i // 3, i % 3].grid(True)

# Adjust spacing between subplots
plt.suptitle("Price and Trend")
plt.tight_layout()

# Show the plots
plt.show()

# Create a figure with two subplots side by side
fig, ax = plt.subplots(6, 1, figsize=(15, 30))

for i, x in enumerate(df.index.year.unique()[1:]):
  temp = transform(str(x), 0)
  ax[i].plot(temp.index, temp['volume'], label='Volume', color='blue')
  ax[i].plot(temp.index, temp['price'], label='Price', color='red')
  # ax[i].plot(temp.index, temp['sen'], label='Volume', color='magenta')
  ax[i].plot(temp.index, temp['trend'], label='Price', color='green')

  ax[i].set(xlabel='Date', ylabel='Scaled Value', title=x)
  ax[i].legend()
  ax[i].grid(True)

# Adjust spacing between subplots
plt.tight_layout()

# Adjust spacing between subplots
plt.subplots_adjust(hspace=0.25)

# Show the plots
plt.show()

df['week'] = df.index.isocalendar().week
df['day'] = df.index.day
df['month'] = df.index.month
df['year'] = df.index.year

dfScaled['week'] = dfScaled.index.isocalendar().week
dfScaled['day'] = dfScaled.index.day
dfScaled['month'] = dfScaled.index.month
dfScaled['year'] = dfScaled.index.year

# Plotty
fig, ax = plt.subplots(nrows=4, ncols=3, figsize=[13, 13], sharey=True)
ax = ax.flatten()
sns_blue = sns.color_palette(as_cmap=True)[0]
MONTHS = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]
YEAR = 2020

for ix, month in enumerate(MONTHS):
    temp1 = dfScaled[(dfScaled['year'] == YEAR) & (dfScaled['month'] == ix + 1)]
    temp = scaler.fit_transform(temp1)
    temp = pd.DataFrame(temp,columns=temp1.columns, index=temp1.index)
    temp.reset_index()["price"].plot(ax=ax[ix], color='r', label = 'Price')
    temp.reset_index()["trend"].plot(ax=ax[ix], color='black', label = 'Trend')
    temp.reset_index()["volume"].plot(ax=ax[ix], color='b', label = 'Volume')
    temp.reset_index()["sen"].plot(ax=ax[ix], color='b', label = 'Sentiment')
    ax[ix].set_title(month)
    ax[ix].legend(loc='upper right')

fig.suptitle(f"Bitcoin Price (USD), by month in {YEAR}")
fig.tight_layout()

# Plot the electricity demand for each day
fig, ax = plt.subplots(nrows=4, ncols=3, figsize=[15, 12], sharey=True)
ax = ax.flatten()
sns_blue = sns.color_palette(as_cmap=True)[0]
MONTHS = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]

for ix, month in enumerate(MONTHS):
    tempM = pd.DataFrame()
    year = df[df['month'] == ix+1]['year'].unique()
    for i in year:
        temp = df[(df['month'] == ix + 1) & (df['year'] == i)]['price']
        temp = pd.DataFrame(temp)

        temp2 = scaler.fit_transform(temp)
        temp2 = pd.DataFrame(temp2,columns=['price'], index=temp.index)
        temp2.reset_index(drop = True, inplace = True)
        temp2["price"].plot(alpha = 0.15, ax=ax[ix], color=sns_blue, label="_no_legend_")
        tempM = pd.concat((tempM, temp2), axis = 1)

    tempM.mean(axis=1).plot(alpha = 1, ax=ax[ix], color=sns_blue, label="Mean")
    ax[ix].legend(loc='upper right')
    ax[ix].set_title(month)


fig.suptitle("Bitcoin Price (USD), by month")
fig.tight_layout()

"""### Outlier

#### Starting Out
"""

# # Create a figure with three subplots in a vertical layout
# fig, axes = plt.subplots(1, 4, figsize=(12, 4))

# for i, x in enumerate(['price', 'volume', 'sen', 'trend']):
#   axes[i].boxplot(df[x], vert=True)
#   axes[i].set_title(f'{x}')
#   axes[i].set_xticklabels([])

# fig.suptitle("Box Plot")
# plt.tight_layout()
# plt.show()

# dfScaled = transform(None, None)

from sklearn.ensemble import IsolationForest
def anomaly_search(df):
  anomaly_inputs = ['price', 'volume']
  model_IF = IsolationForest(contamination=float(0.1),random_state=42069)
  model_IF.fit(df[anomaly_inputs])
  df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])
  df['anomaly'] = model_IF.predict(df[anomaly_inputs])
  return df

def outlier_plot(data, outlier_method_name, x_var, y_var,
                 xaxis_limits=[0,1], yaxis_limits=[0,1]):

    print(f'Outlier Method: {outlier_method_name}')

    # Create a dynamic title based on the method
    method = f'{outlier_method_name}_anomaly'

    # Print out key statistics
    print(f"Number of anomalous values {len(data[data['anomaly']==1])}")
    print(f"Number of non anomalous values  {len(data[data['anomaly']==0])}")
    print(f'Total Number of Values: {len(data)}\n')

    # Create the chart using seaborn
    g = sns.FacetGrid(data, col='anomaly', height=4, hue='anomaly', hue_order=[0,1])
    g.map(sns.scatterplot, x_var, y_var)
    g.fig.suptitle(f'Outlier Method: {outlier_method_name}', y=1.10, fontweight='bold')
    g.set(xlim=xaxis_limits, ylim=yaxis_limits)
    axes = g.axes.flatten()
    axes[0].set_title(f"Outliers\n{len(data[data['anomaly']== 1])} points")
    axes[1].set_title(f"Inliers\n {len(data[data['anomaly']== 0])} points")
    plt.show()
    return g

df = anomaly_search(df)
df['anomaly'] = df['anomaly'].map({1:0, -1:1})

# Scatter plot with colors based on the 'anomaly' column
scatter = plt.scatter(df['price'], df['volume'], c=df['anomaly'], cmap='seismic', edgecolors='white', linewidths=.4)

# Adding labels and title
plt.xlabel('Harga')
plt.ylabel('Volume')
plt.title('Outlier Detection using Isolation Forest')

# Create a legend using scatter object and its mapping
legend_labels = {0: 'Inlier', 1: 'Outlier'}
plt.legend(handles=scatter.legend_elements()[0], labels=legend_labels.values(), loc='upper right')

# Show the plot
plt.show()

outlier_plot(df, "Isolation Forest", 'price', 'volume', [df['price'].min(),df['price'].max()], [df['volume'].min(),df['volume'].max()])

palette = ['#ff7f0e', '#1f77b4']
anomaly_inputs = ['price', 'volume']
sns.pairplot(df, vars=anomaly_inputs, hue='anomaly', palette=palette)

"""#### Handling"""

def moving_average(df, column, window = 24):
  i = 0
  res = []
  for i in np.arange(len(df.iloc[window:len(df)])):
    res.append(df[column].iloc[i:i+window].mean())
  return res

import pandas as pd

# Set the mode to suppress the warning
pd.options.mode.chained_assignment = None  # or 'warn' to show the warning, or 'raise' to raise an exception

# Your code that generates the SettingWithCopyWarning

# Reset the mode to the default value if needed
# pd.options.mode.chained_assignment = 'warn'  # or 'raise' to revert to the default behavior

dfAnomaly  = df[df['anomaly'] == 1]
dfAnomaly = dfAnomaly[dfAnomaly.index < df.iloc[int(len(df)*.8)].name].sort_values(by = 'anomaly_scores', ascending = False)
dfAnomaly = dfAnomaly.iloc[:int(len(dfAnomaly)*.1)]

df['price'].plot(label='data')
# avgPrice.plot(label='mean')
df.loc[dfAnomaly.index, 'price'].plot(label='outliers', marker='o', ls='')
plt.grid()
plt.legend()

dfAnomaly

anMean = []
for i in np.arange(50):
  i+=1
  df = anomaly_search(df)
  df['anomaly'] = df['anomaly'].map({1:0, -1:1})
  dfAnomaly  = df[df['anomaly'] == 1]
  lenWin = i
  rollPrice  = moving_average(df, 'price' , lenWin)
  rollVolume = moving_average(df, 'volume', lenWin)

  df['repPrice']  = np.nan
  df['repVolume'] = np.nan
  df['repVolume'].iloc[lenWin:len(df)] = rollVolume
  df['repPrice'].iloc[lenWin:len(df)] = rollPrice

  dfAnomaly = dfAnomaly[dfAnomaly['anomaly_scores'] < -0.1]

  df2 = df.copy()
  df2.loc[dfAnomaly.index, 'price']  =  df2['repPrice'].loc[dfAnomaly.index].values
  df2.loc[dfAnomaly.index, 'volume'] = df2['repVolume'].loc[dfAnomaly.index].values

  df2 = anomaly_search(df2[['price', 'volume']])
  dfAnomaly  = df2[df2['anomaly'] == 1]
  anMean.append(dfAnomaly['anomaly_scores'].sum())

iForest = pd.DataFrame()
iForest['window'] = np.arange(1, 51, 1)
iForest['anMean'] = anMean
iForest.set_index('window', inplace = True)

iForest.sort_values(by = 'anMean', inplace = True)
iForest.head()
# iForest.to_csv('iForest_window.csv')

dfPrice = df[['price']].copy()

dfPrice.head()

df = anomaly_search(df)
df['anomaly'] = df['anomaly'].map({1:0, -1:1})
dfAnomaly  = df[df['anomaly'] == 1]

lenWin = 50

rollPrice  = moving_average(df, 'price' , lenWin)
rollVolume = moving_average(df, 'volume', lenWin)

df['repPrice']  = np.nan
df['repVolume'] = np.nan
df['repVolume'].iloc[lenWin:len(df)] = rollVolume
df['repPrice'].iloc[lenWin:len(df)] = rollPrice

dfAnomaly = dfAnomaly[dfAnomaly['anomaly_scores'] < -0.1]

df2 = df.copy()
df2.loc[dfAnomaly.index, 'price']  =  df2['repPrice'].loc[dfAnomaly.index].values
df2.loc[dfAnomaly.index, 'volume'] = df2['repVolume'].loc[dfAnomaly.index].values

df['price'].plot(label='data')
# avgPrice.plot(label='mean')
df.loc[dfAnomaly.index, 'price'].plot(label='outliers', marker='o', ls='')
df.loc[dfAnomaly.index, 'repPrice'].plot(label='replacement', marker='o', ls='', color = 'blue')
plt.legend()

df['volume'].plot(label='data')
# avgPrice.plot(label='mean')
df.loc[dfAnomaly.index, 'volume'].plot(label='outliers', marker='o', ls='')
df.loc[dfAnomaly.index, 'repVolume'].plot(label='replacement', marker='o', ls='', color = 'blue')
plt.legend()

df2 = df.copy()
df2.loc[dfAnomaly.index, 'price']  =  df2['repPrice'].loc[dfAnomaly.index].values
df2.loc[dfAnomaly.index, 'volume'] = df2['repVolume'].loc[dfAnomaly.index].values

df2

# df2.drop(columns=['week', 'day', 'month', 'year', 'anomaly_scores', 'anomaly'], inplace = True, axis = 1)
df2[['price', 'sen', 'volume', 'trend']].to_csv('dataClean10_8.csv')

df2 = anomaly_search(df2)
outlier_plot(df2, "Isolation Forest", 'price', 'volume', [df2['price'].min(),df2['price'].max()], [df2['volume'].min(),df2['volume'].max()])

dfAnomaly = df2[df2['anomaly'] == -1]

df2['price'].plot(label='data')
avgPrice.plot(label='mean')
df2.loc[dfAnomaly.index, 'price'].plot(label='outliers', marker='o', ls='')
# avg[dfAnomaly.index].plot(label='replacement', marker='o', ls='', color = 'blue')
plt.legend()
plt.show()

dfAnomaly = dfAnomaly[dfAnomaly['anomaly_scores'] < -0.1]

df2['price'].plot(label='data')
avgPrice.plot(label='mean')
df2.loc[dfAnomaly.index, 'price'].plot(label='outliers', marker='o', ls='')
avgPrice[dfAnomaly.index].plot(label='replacement', marker='o', ls='', color = 'blue')
plt.legend()
plt.show()

"""## Final Result"""

df2.drop(columns = ['anomaly_scores', 'anomaly'], axis = 1, inplace = True)
df2.to_csv("finalData2.csv")

from sklearn.model_selection import train_test_split

train_data, val_data = train_test_split(df2, test_size=0.2, shuffle=False)

# Line plot for training and validation data
plt.figure(figsize=(10, 6))
plt.plot(train_data.index, train_data['price'], label='Data Latih', color='blue')
plt.plot(val_data.index, val_data['price'], label='Data Uji', color='red')

plt.title('Harga Bitcoin')
plt.xlabel('Waktu')
plt.ylabel('Harga')
plt.legend()
plt.show()

"""# Experiment!"""

dfSenCount2 = dfSen[dfSen['sen'] != 0]
dfSenCount2 = dfSenCount2.groupby('date').mean()
scaler = MinMaxScaler(feature_range=(-1, 1))
dfSenCount2['sen'] = scaler.fit_transform(dfSenCount2)

# Create a date range covering the entire period
start_date = dfSenCount.index.min()
end_date = dfSenCount.index.max()
date_range = pd.date_range(start_date, end_date)

# Reindex the DataFrame to include missing dates
dfSenCount2 = dfSenCount2.reindex(date_range)

# Fill missing values with the mean of the surrounding data
dfSenCount2.interpolate(method = 'linear', inplace = True)
# dfSenCount2['sen'] = dfSenCount2['sen'].fillna((dfSenCount2['sen'].shift() + dfSenCount2['sen'].shift(-1)) / 2)

df2 = pd.concat([dfPrice[['price', 'volume']], dfSenCount2, dfTrends], axis = 1)
df2.index.name = 'date'

"""### Outlier"""

# # Create a figure with three subplots in a vertical layout
# fig, axes = plt.subplots(1, 4, figsize=(12, 4))

# for i, x in enumerate(['price', 'volume', 'sen', 'trend']):
#   axes[i].boxplot(df[x], vert=True)
#   axes[i].set_title(f'{x}')
#   axes[i].set_xticklabels([])

# fig.suptitle("Box Plot")
# plt.tight_layout()
# plt.show()

dfScaled = transform(None, None)

from sklearn.ensemble import IsolationForest
def anomaly_search(df):
  anomaly_inputs = ['price', 'volume', 'sen', 'trend']
  model_IF = IsolationForest(contamination=float(0.1),random_state=42069)
  model_IF.fit(df[anomaly_inputs])
  df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])
  df['anomaly'] = model_IF.predict(df[anomaly_inputs])
  return df

def outlier_plot(data, outlier_method_name, x_var, y_var,
                 xaxis_limits=[0,1], yaxis_limits=[0,1]):

    print(f'Outlier Method: {outlier_method_name}')

    # Create a dynamic title based on the method
    method = f'{outlier_method_name}_anomaly'

    # Print out key statistics
    print(f"Number of anomalous values {len(data[data['anomaly']==-1])}")
    print(f"Number of non anomalous values  {len(data[data['anomaly']== 1])}")
    print(f'Total Number of Values: {len(data)}\n')

    # Create the chart using seaborn
    g = sns.FacetGrid(data, col='anomaly', height=4, hue='anomaly', hue_order=[1,-1])
    g.map(sns.scatterplot, x_var, y_var)
    g.fig.suptitle(f'Outlier Method: {outlier_method_name}', y=1.10, fontweight='bold')
    g.set(xlim=xaxis_limits, ylim=yaxis_limits)
    axes = g.axes.flatten()
    axes[0].set_title(f"Outliers\n{len(data[data['anomaly']== -1])} points")
    axes[1].set_title(f"Inliers\n {len(data[data['anomaly']==  1])} points")
    plt.show()
    return g

df2 = anomaly_search(df2)
outlier_plot(df2, "Isolation Forest", 'price', 'volume', [df2['price'].min(),df2['price'].max()], [df2['volume'].min(),df2['volume'].max()])

palette = ['#ff7f0e', '#1f77b4']
anomaly_inputs = ['price', 'volume', 'sen', 'trend']
sns.pairplot(df2, vars=anomaly_inputs, hue='anomaly', palette=palette)

"""#### Handling"""

dfAnomaly = df2[df2['anomaly'] == -1]
lenWin = 24

rollPrice  = df2['price'].rolling(window=lenWin, min_periods=1, center=True)
rollVolume = df2['volume'].rolling(window=lenWin, min_periods=1, center=True)
rollSen  = df2['sen'].rolling(window=lenWin, min_periods=1, center=True)
rollTrend = df2['trend'].rolling(window=lenWin, min_periods=1, center=True)
avgPrice  = rollPrice.mean()
avgVolume = rollVolume.mean()
avgSen = rollSen.mean()
avgTrend = rollTrend.mean()

df2['price'].plot(label='data')
avgPrice.plot(label='mean')
df2.loc[dfAnomaly.index, 'price'].plot(label='outliers', marker='o', ls='')
# avg[dfAnomaly.index].plot(label='replacement', marker='o', ls='', color = 'blue')
plt.legend()
plt.show()

dfAnomaly = dfAnomaly[dfAnomaly['anomaly_scores'] < -0.1]

df2['price'].plot(label='data')
avgPrice.plot(label='mean')
df2.loc[dfAnomaly.index, 'price'].plot(label='outliers', marker='o', ls='')
avgPrice[dfAnomaly.index].plot(label='replacement', marker='o', ls='', color = 'blue')
plt.legend()

print(len(dfAnomaly))
print(len(df2[df2['anomaly'] == -1]))

df2 = df2.copy()
df2.loc[dfAnomaly.index, 'price']  = avgPrice.loc[dfAnomaly.index].values
df2.loc[dfAnomaly.index, 'volume'] = avgVolume.loc[dfAnomaly.index].values
df2.loc[dfAnomaly.index, 'sen'] = avgSen.loc[dfAnomaly.index].values
df2.loc[dfAnomaly.index, 'trend'] = avgTrend.loc[dfAnomaly.index].values

# df2.drop(columns=['week', 'day', 'month', 'year', 'anomaly_scores', 'anomaly'], inplace = True)
# df2.to_csv('finalData2.csv')

df2 = anomaly_search(df2)
outlier_plot(df2, "Isolation Forest", 'price', 'volume', [df2['price'].min(),df2['price'].max()], [df2['volume'].min(),df2['volume'].max()])

dfAnomaly = df2[df2['anomaly'] == -1]

df2['price'].plot(label='data')
avgPrice.plot(label='mean')
df2.loc[dfAnomaly.index, 'price'].plot(label='outliers', marker='o', ls='')
# avg[dfAnomaly.index].plot(label='replacement', marker='o', ls='', color = 'blue')
plt.legend()
plt.show()

dfAnomaly = dfAnomaly[dfAnomaly['anomaly_scores'] < -0.1]

df2['price'].plot(label='data')
avgPrice.plot(label='mean')
df2.loc[dfAnomaly.index, 'price'].plot(label='outliers', marker='o', ls='')
avgPrice[dfAnomaly.index].plot(label='replacement', marker='o', ls='', color = 'blue')
plt.legend()
plt.show()

df2.drop(['anomaly_scores', 'anomaly'], axis = 1, inplace = True)
df2.to_csv('dataExperiment.csv')

df2

"""## Feature Importance"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
# %matplotlib inline

X = df2.iloc[:,1:].reset_index(drop = True).copy()
estimators=[]
estimators.append(['minmax',MinMaxScaler(feature_range=(-1,1))])
scale=Pipeline(estimators)
X=scale.fit_transform(X)
pca = PCA(n_components=3,random_state=42069)
pca.fit(X)

np.cumsum(pca.explained_variance_ratio_)

#Plotting the Cumulative Summation of the Explained Variance
plt.figure()
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Variance (%)') #for each component
plt.title('Explained Variance')
plt.show()

print(f"\n{pca.explained_variance_ratio_}")

df_pca=pd.DataFrame(pca.components_).transpose()
df_pca.columns=df2.iloc[:,1:].columns
y=df2.iloc[:,1:2]
y.reset_index(drop=True,inplace=True)
df_pca['priceUSD']=y
df_pca.head()

"""# Parameter Final"""

DIR = "./Skripsi/gridSearchNLP/"

List = ['256GT', '256GTH', '256T',
        '128GT', '128GTH', '128T',
        '128_2GT', '128_2GTH', '128_2T',
        '256_2GT', '128_2GTH', '128_2T'
       ]

dfList = []
dfFinal = pd.DataFrame()
for i, x in enumerate(List):
    globals()[f'df{i}'] = pd.read_csv(DIR+f"result_{x}.csv")
    globals()[f'df{i}']['type'] = x
    del globals()[f'df{i}']['Unnamed: 0']
    globals()[f'df{i}'].reset_index(inplace = True, drop = True)
    dfFinal = pd.concat([dfFinal, globals()[f'df{i}']])
    dfList.append(globals()[f'df{i}'])

dfFinal.reset_index(inplace = True, drop = True)
dfFinal.set_index('type', inplace = True)
dfTop = dfFinal.sort_values(by = 'testAccuracy', ascending = False).head(10)

print(dfTop)
dfTop.to_csv('paramsSenFinale.csv')

"""# Final Parameter"""

dfC10.iloc[50]

dfC10.iloc[49]

DIR = './Skripsi/gridSearchResult/Phase 2/'
dfExp = pd.read_csv(DIR + "resultExperiment.csv")
temp1 = pd.read_csv(DIR + "resultExperiment-1.csv")
temp2 = pd.read_csv(DIR + "resultExperiment-2.csv")
dfExp = pd.concat([dfExp, temp1, temp2])
dfExp.reset_index(inplace = True, drop = True)
print(len(dfExp))

dfC10 = pd.read_csv(DIR + "resultClean10.csv")
temp = pd.read_csv(DIR + "resultClean10-1.csv")
dfC10 = pd.concat([dfC10, temp])
dfC10.reset_index(inplace = True, drop = True)
print(len(dfC10))

dfRaw = pd.read_csv(DIR + "resultRaw.csv")
temp1 = pd.read_csv(DIR + "resultRaw-1.csv")
temp2 = pd.read_csv(DIR + "resultRaw-2.csv")
dfRaw = pd.concat([dfRaw, temp1, temp2])
dfRaw.reset_index(inplace = True, drop = True)
print(len(dfRaw))

dfRaw.sort_values(by = 'sMAPE2').head()

dfC10.sort_values(by = 'sMAPE2').head()

dfExp.sort_values(by = 'sMAPE2').head()

"""# Trendy Analysis"""

!git clone https://github.com/joaofig/pyloess

import sys
sys.path.append('./pyloess/pyloess/')

# Now you can import modules from the specified directory
from Loess import Loess

loess = Loess(df['sen'], np.arange(len(df['sen'])))

for x in df['sen']:
    y = loess.estimate(x, window=365, use_matrix=False, degree=1)

from statsmodels.nonparametric.smoothers_lowess import lowess

filtered = lowess(np.arange(len(df['sen'])), df['sen'], is_sorted=True, frac=.9, it=14)

# plt.plot(df.index, np.arange(len(df['sen'])), 'r')
plt.plot(filtered[:,0], filtered[:,1], 'b')

plt.show()

xx = np.array([0.5578196, 2.0217271, 2.5773252, 3.4140288, 4.3014084,
                4.7448394, 5.1073781, 6.5411662, 6.7216176, 7.2600583,
                8.1335874, 9.1224379, 11.9296663, 12.3797674, 13.2728619,
                4.2767453, 15.3731026, 15.6476637, 18.5605355, 18.5866354,
                18.7572812])
yy = np.array([18.63654, 103.49646, 150.35391, 190.51031, 208.70115,
                213.71135, 228.49353, 233.55387, 234.55054, 223.89225,
                227.68339, 223.91982, 168.01999, 164.95750, 152.61107,
                160.78742, 168.55567, 152.42658, 221.70702, 222.69040,
                243.18828])

loess = Loess(xx, yy)

for x in xx:
    y = loess.estimate(x, window=7, use_matrix=False, degree=1)
    print(x, y)

plt.plot(x, y)

from loess.loess_1d import loess_1d

xout, yout, wout = loess_1d(df['sen'], np.arange(len(df['sen'])), xnew=None, degree=7, frac=0.9)

plt.plot(xout)

import numpy as np
import matplotlib.pyplot as plt

from loess.loess_1d import loess_1d

def loess_1d_example():
    """ Usage example for loess_1d """

    n = 50
    nbad = int(n*0.1) # 10% outliers
    np.random.seed(5)

    # Compute the true model
    xtrue = np.random.uniform(-np.pi, np.pi, n - nbad)
    xtrue.sort()   # Sort only for plotting smooth lines
    ytrue = np.sin(xtrue) + xtrue/2

    # Add noise to the data
    sigy = 0.5
    yran = np.random.normal(ytrue, sigy)

    # Add outliers to the data
    xbad = np.random.uniform(-np.pi, np.pi, nbad)
    ybad = np.random.uniform(-5, 5, nbad)
    xfit = np.append(xtrue, xbad)
    yfit = np.append(yran, ybad)

    # Sort only for plotting smooth lines
    j = np.argsort(xfit)
    xfit, yfit = xfit[j], yfit[j]

    # LOESS smoothing on the same coordinates as input
    xout1, yout1, weights1 = loess_1d(xfit, yfit, frac=0.8, degree=2)

    # LOESS smoothing on a new output grid (xnew)
    xnew = np.linspace(np.min(xfit), np.max(xfit), 30)
    xout2, yout2, weights2 = loess_1d(xfit, yfit, xnew, frac=0.8, degree=2)

    plt.clf()
    plt.subplot(211)
    plt.plot(xfit, yfit, 'ro', label='Noisy')
    plt.plot(xtrue, ytrue, color='limegreen', label='True')
    plt.plot(xout1, yout1, '+-b', label='LOESS')
    w = weights1 < 0.34  # identify outliers
    plt.plot(xfit[w], yfit[w], 'xk', ms=10, label='Outliers')
    plt.title("LOESS smoothing at the input coordinates")
    plt.legend()

    plt.subplot(212)
    plt.plot(xfit, yfit, 'ro', label='Noisy')
    plt.plot(xtrue, ytrue, color='limegreen', label='True')
    plt.plot(xout2, yout2, '+-b', label='LOESS')
    plt.title("LOESS smoothing on a regular output grid")
    plt.legend()

    plt.tight_layout()
    plt.pause(1)

#-----------------------------------------------------------------------------

if __name__ == '__main__':

    loess_1d_example()

df = pd.read_csv("./Skripsi/finalData/dataRaw.csv", index_col = ['date'], parse_dates = ['date'])

from statsmodels.tsa.seasonal import MSTL
stl_kwargs = {"seasonal_deg": 0}

model = MSTL(df['sen'], periods=(365), stl_kwargs=stl_kwargs)
res = model.fit()

df['trendSmooth'] = res.trend

df['sen'] = df['trendSmooth'].copy()
df.drop('sen', axis = 1, inplace = True)
df.to_csv("dataExperiment.csv")

from statsmodels.tsa.seasonal import MSTL
stl_kwargs = {"seasonal_deg": 0}

model = MSTL(df['price'], periods=(365), stl_kwargs=stl_kwargs)
res = model.fit()

df['priceSmooth'] = res.trend

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit_transform(df[['trendSmooth']])
senScaled = scaler.transform(df[['trendSmooth']])

scaler.fit_transform(df[['priceSmooth']])
priceScaled = scaler.transform(df[['priceSmooth']])

dfPlot = pd.DataFrame(index = df.index)
dfPlot["senScaled"] = senScaled
dfPlot["priceScaled"] = priceScaled

# plt.plot(senScaled)
plt.plot(dfPlot["priceScaled"], label = 'Price')
plt.plot(dfPlot["senScaled"], label = 'Sentiment')
plt.title('Comparing Bitcoin Price and Sentiment smoothed with MSTL')
plt.xlabel("Year")
plt.ylabel("Value (Normalized)")
plt.legend()
plt.show()

df

"""# Data Augmentation Again"""

plt.plot(df['price'])

percentile25 = df['price'].quantile(0.25)
percentile75 = df['price'].quantile(0.75)

print(percentile25)
print(percentile75)

df['price'].describe()

outlier_treatment(df['price'])

"""# Oke Fix Final"""

import pandas as pd
!rm -r "./Skripsi"
!git clone https://github.com/AfterRain007/Skripsi

var1 = ['del',
        'only']

var2 = ['Trend',
        'Vol',
        'Sen']

result = pd.DataFrame()

sortBy = 'MAPE'

for x in var1:
  for y in var2:
    temp = pd.read_csv(f"./Skripsi/gridSearchResult/dfResultClean10TFT_{x}{y}.csv")
    temp.sort_values(by = sortBy, inplace = True)
    temp['Included Covariate'] = x + y
    temp.set_index('Included Covariate', inplace = True)
    result = pd.concat([result, temp.head(1)])

temp = pd.read_csv('./Skripsi/gridSearchResult/dfResultClean10TFT_onlyPrice.csv')
temp.sort_values(by = sortBy, inplace = True)
temp['Included Covariate'] = 'onlyPrice'
temp.set_index('Included Covariate', inplace = True)
result = pd.concat([result, temp.head(1)])

temp = pd.read_csv('./Skripsi/gridSearchResult/dfResultClean10TFT.csv')
temp.sort_values(by = sortBy, inplace = True)
temp['Included Covariate'] = 'Complete'
temp.set_index('Included Covariate', inplace = True)
result = pd.concat([result, temp.head(1)])
result.sort_index(inplace = True)
# result.sort_values(by = 'RMSE', inplace = True)

new_index_names = {'Complete': 'Trend, Sentiment, and Volume',
                   'delSen': 'Trend and Volume',
                   'delTrend': 'Sentiment and Volume',
                   'delVol': 'Trend and Sentiment',
                   'onlySen': 'Sentiment',
                   'onlyTrend': 'Trend',
                   'onlyVol': 'Volume',
                   'onlyPrice': 'Price'
                   }

result = result.rename(index=new_index_names)

result.head(1)

result[['RMSE', 'sMAPE', 'MAPE']].sort_values(by = 'RMSE')

result[['RMSE', 'sMAPE', 'MAPE']]

result[['RMSE', 'sMAPE', 'MAPE']]

"""# Some other finale idk im out of words"""

dfResult = pd.DataFrame()
data = ['30', 'MAnP']
model = ['TFT', 'LSTM', 'TCN', 'GRU']

for x in data:
  for y in model:
    temp = pd.read_csv(f"./Skripsi/evalTS Diff4/result{y}_{x}.csv")
    temp.sort_values(by = 'RMSE', inplace = True)
    temp['type'] = y+x
    dfResult = pd.concat([dfResult, temp.head(3)])

dfResult.set_index('type', inplace = True)

dfResult.sort_values(by = "RMSE")[['RMSE', "MAE", "MAPE"]].to_excel("resultDiff4MA.xlsx")

dfResult.sort_values(by = "RMSE")

dfResult = pd.DataFrame()
data = ['BP', 'BPnP']
model = ['TFT', 'LSTM', 'TCN', 'GRU']

for x in data:
  for y in model:
    temp = pd.read_csv(f"./Skripsi/evalTS Diff4/result{y}_{x}.csv")
    temp.sort_values(by = 'RMSE', inplace = True)
    temp['type'] = y+x
    dfResult = pd.concat([dfResult, temp.head(1)])

# y = 'TFT'
# x = 'MAonlyTrend'
# temp = pd.read_csv(f"./Skripsi/evalTS Diff4/result{y}_{x}.csv")
# temp.sort_values(by = 'RMSE', inplace = True)
# temp['type'] = y+x
# dfResult = pd.concat([dfResult, temp.head(1)])

# x = 'MAexperiment'
# temp = pd.read_csv(f"./Skripsi/evalTS Diff4/result{y}_{x}.csv")
# temp.sort_values(by = 'RMSE', inplace = True)
# temp['type'] = y+x
# dfResult = pd.concat([dfResult, temp.head(1)])

# x = '30WOCovariates'
# temp = pd.read_csv(f"./Skripsi/evalTS Diff4/result{y}_{x}.csv")
# temp.sort_values(by = 'RMSE', inplace = True)
# temp['type'] = y+x
# dfResult = pd.concat([dfResult, temp.head(1)])

# x = '30WOAnyCovariates'
# temp = pd.read_csv(f"./Skripsi/evalTS Diff4/result{y}_{x}.csv")
# temp.sort_values(by = 'RMSE', inplace = True)
# temp['type'] = y+x
# dfResult = pd.concat([dfResult, temp.head(1)])

dfResult.set_index('type', inplace = True)

dfResult.sort_values(by = "RMSE")[['RMSE', "MAE", "MAPE"]].to_excel("resultDiff4BP.xlsx")

dfResult = pd.DataFrame()
y = 'TFT'
x = 'MAonlyTrend'
temp = pd.read_csv(f"./Skripsi/evalTS Diff4/result{y}_{x}.csv")
temp.sort_values(by = 'RMSE', inplace = True)
temp['type'] = y+x
dfResult = pd.concat([dfResult, temp.head(1)])

x = 'MAexperiment'
temp = pd.read_csv(f"./Skripsi/evalTS Diff4/result{y}_{x}.csv")
temp.sort_values(by = 'RMSE', inplace = True)
temp['type'] = y+x
dfResult = pd.concat([dfResult, temp.head(1)])

x = '30WOCovariates'
temp = pd.read_csv(f"./Skripsi/evalTS Diff4/result{y}_{x}.csv")
temp.sort_values(by = 'RMSE', inplace = True)
temp['type'] = y+x
dfResult = pd.concat([dfResult, temp.head(1)])

x = '30WOAnyCovariates'
temp = pd.read_csv(f"./Skripsi/evalTS Diff4/result{y}_{x}.csv")
temp.sort_values(by = 'RMSE', inplace = True)
temp['type'] = y+x
dfResult = pd.concat([dfResult, temp.head(1)])

dfResult.set_index('type', inplace = True)

dfResult.sort_values(by = "RMSE")[['RMSE', "MAE", "MAPE"]].to_excel("resultDiff4Experiment.xlsx")

timeTFT = pd.read_excel(f"./Skripsi/evalTS Diff4/timeTFT.xlsx")

timeTFT

df = pd.read_excel("./Skripsi/backTestResult.xlsx")

dfResult[:13][['RMSE', "MAE", "MAPE"]]

dfResult = pd.DataFrame()
dfName = ["MA", "MAnP", "BP", "BPnP", "MAexperiment", "MAonlyTrend"]

for i, x in enumerate(dfName):
    temp = pd.read_csv(f"./Skripsi/resultTFT_{x}.csv")
    temp['data'] = x
    temp.set_index("data", inplace = True)
    temp.sort_values(by = 'RMSE', inplace = True)
    dfResult = pd.concat([dfResult, temp.head(5)])

dfResult.drop(["Unnamed: 0", 'type'], axis = 1, inplace = True)
# dfResult.sort_values(by = 'RMSE', inplace = True)
len(dfResult)

dfResult = pd.read_csv(f"./Skripsi/resultTFT_MAexperiment.csv")

dfResult.sort_values(by = 'RMSE')

dfRaw = pd.read_csv("./Skripsi/finalData/dataRaw.csv", usecols=['date', 'sen'], parse_dates=['date'], index_col=['date'])
dfExperiment = pd.read_csv("./Skripsi/finalData/dataExperiment2.csv", usecols=['date', 'trendSmooth'], parse_dates=['date'], index_col=['date'])

fig, ax = plt.subplots(1,2, figsize=(15, 6))

ax[0].plot(dfRaw)
ax[0].set_title("Raw")

ax[1].plot(dfExperiment)
ax[1].set_title("Experiment")

plt.show()

import pandas as pd
!pip install opendatasets -q
import opendatasets as od
import json
import gc

! cp ./Skripsi/Trash/kaggle.json ./
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
# Download datasets using opendatasets
od.download("https://www.kaggle.com/datasets/kaushiksuresh147/bitcoin-tweets")
od.download("https://www.kaggle.com/datasets/afterrain007/reddit-bitcoin-topic-text")

# reading the CSV file
df = pd.read_csv('./bitcoin-tweets/Bitcoin_tweets.csv', usecols = ['date', 'text'], dtype = {"date": str, "text" : str})
df['date'] = pd.to_datetime(df['date'], errors='coerce')
df.dropna(inplace = True)
gc.collect()
df.head()

df2 = pd.read_csv('./reddit-bitcoin-topic-text/redditText.csv')

dfAll = pd.concat([df, df2])

len(dfAll)

df.set_index('date', inplace = True)

df.loc["2021-12"]

df.sort_values(by = 'date', inplace = True)

df.head(1)

df.tail(5)

import pandas as pd
import numpy as np
shutil.rmtree("./Skripsi")
!git clone https://github.com/AfterRain007/Skripsi

df = pd.read_csv("./Skripsi/resultTFT_MAonlyTrend.csv")

# df = df[df['full_attention'] == False].copy()
df.sort_values(by = 'RMSE', inplace = True)
df.head(10)

dfResult

type = "TFT"

dfList2 = ['BP', 'MA']
dfList3 = ['', 'nP']

dfResult = pd.DataFrame()

for x in dfList2:
    for y in dfList3:
        temp = pd.read_csv(f"./Skripsi/result{type}_{x}{y}.csv")
        temp.sort_values(by = 'RMSE', inplace = True)
        temp['type'] = x+y
        temp.set_index('type', inplace = True)
        dfResult = pd.concat([dfResult, temp.head(1)])

dfResult

timeModel = []
for index, row in dfResult.iterrows():
    df = pd.read_csv("C:/Users/Razel/Documents/GitHub/Skripsi/finalData/dataRaw.csv", parse_dates=['date'], index_col = ['date'], usecols = ['date',  'price', 'trend', 'volume', 'sen'])

    if index[:2] == "MA":
        df2 = replaceOutlierMA(df, 30)
    else:
        df2 = replaceOutlierBP(df, 'price')
        df2 = replaceOutlierBP(df2, 'volume')

    if adfuller(df2['price'])[1] > 0.05:
        df2['price'] = df2['price'].diff()

    if adfuller(df2['volume'])[1] > 0.05:
        df2['volume'] = df2['volume'].diff()

    df2 = df2.iloc[1:].copy()

    ts_ttrain, ts_ttest, cov_t = createTimeseries(df2)

    if index[2:] == "nP":
        cov_t = cov_t[['day', 'week', 'month', 'year', 'linear_increase', 'holidays']].astype(np.float32)
    else:
        cov_t = cov_t.astype(np.float32)

    # throughout training we'll monitor the validation loss for both pruning and early stopping
    my_stopper = EarlyStopping(
        monitor="val_loss",
        patience=10,
        min_delta=0.01,
        mode='min',)

    callbacks = [my_stopper]

    pl_trainer_kwargs = {
        "accelerator": "auto",
        "callbacks": callbacks,
    }

    # reproducibility
    np.random.seed(42069)
    torch.manual_seed(42069)

    # build the TFT model
    model = TFTModel(
        input_chunk_length=row['in_len'],
        output_chunk_length=row['out_len'],
        hidden_size=int(row['hidden_size']),
        lstm_layers=row['lstm_layers'],
        num_attention_heads=row['num_attention_heads'],
        dropout=row['dropout'],
        hidden_continuous_size=row['hidden_continuous_size'],
        use_static_covariates=True,
        batch_size=int(row['batch_size']),
        optimizer_kwargs={'lr': row['lr']},
        n_epochs=500,
        nr_epochs_val_period=10,
        likelihood=None,
        loss_fn=torch.nn.MSELoss(),
        full_attention=row['full_attention'],
        torch_metrics=MeanAbsolutePercentageError(),
        random_state=42069,
        force_reset=True,
        pl_trainer_kwargs=pl_trainer_kwargs,  # Assuming pl_trainer_kwargs is defined elsewhere
        add_relative_index=False,
    )

    start = time.time()

    # train the model
    model.fit(ts_ttrain,                    # Train Price
              val_series=ts_ttest,          # Val Price
              future_covariates=cov_t,      # Val Covariate
              val_future_covariates=cov_t,  # Val Covariate
              verbose=True)

    trainingTime = time.time() - start

    start = time.time()
    pred = model.predict(len(ts_ttest))
    predictingTime = time.time() - start

    timeModel.append([index, trainingTime, predictingTime])

dfTime = pd.DataFrame(timeModel, columns = ['type', 'Training', 'Prediction'])
dfTime.set_index('type', inplace = True)
dfTime['Total Time'] = dfTime.sum(axis = 1)
dfTime

dfList = ['TFT', 'TCN', 'GRU', 'LSTM']
dfList2 = ['', 'nP']
dfList3 = ['BP', 'MA']
df = pd.DataFrame()

for x in dfList2:
  for y in dfList:
    for z in dfList3:
      temp = pd.read_csv(f"./Skripsi/result{y}_{z}{x}.csv")
      temp.sort_values(by = 'RMSE', inplace = True)
      temp['tipe'] = y+z+x
      df = pd.concat([df, temp.head(1)])

df.reset_index(drop = True, inplace = True)
df.drop('Unnamed: 0', axis = 1, inplace = True)
# df['tipe'] = df['tipe'].replace({'TCNnP': 'TCN (Tanpa Variabel Penjelas)', 'TFTnP': 'TFT (Tanpa Variabel Penjelas)',
#                                  'GRUnP': 'GRU (Tanpa Variabel Penjelas)', 'LSTMnP': 'LSTM (Tanpa Variabel Penjelas)'})

# df = df[['tipe', 'RMSE', 'MAE', 'MAPE']].copy()
# df.sort_values(by = "RMSE", inplace = True)

df.to_excel("resultMovingAverage.xlsx", index = False)
df

dfList = ['TFT', 'TCN', 'GRU', 'LSTM']
dfList2 = ['', 'nP']
df = pd.DataFrame()

for x in dfList2:
  for y in dfList:
    temp = pd.read_csv(f"./Skripsi/result{y}_BP{x}.csv")
    temp.sort_values(by = 'RMSE', inplace = True)
    temp['tipe'] = y+x
    df = pd.concat([df, temp.head(1)])

df.reset_index(drop = True, inplace = True)
df['tipe'] = df['tipe'].replace({'TCNnP': 'TCN (Tanpa Variabel Penjelas)', 'TFTnP': 'TFT (Tanpa Variabel Penjelas)',
                                 'GRUnP': 'GRU (Tanpa Variabel Penjelas)', 'LSTMnP': 'LSTM (Tanpa Variabel Penjelas)'})

df = df[['tipe', 'RMSE', 'MAE', 'MAPE']].copy()
# df.sort_values(by = "RMSE", inplace = True)

df.to_excel("resultBoxPlot.xlsx", index = False)
df

df = pd.read_csv("./Skripsi/dataRaw.csv", parse_dates=['date'], index_col='date')

df['price'] = df['price'].diff()
df['volume'] = df['volume'].diff()
df = df.iloc[1:].copy()

fig, ax = plt.subplots(2,2, figsize=(15, 8))

for i, x in enumerate(df.columns):
  ax[i%2, (i // 2) % 2].plot(df.index, df[x])
  ax[i%2, (i // 2) % 2].set_title(x.capitalize())

fig.suptitle('Line Plot of All Variable')

plt.tight_layout()
plt.show()

plt.plot(df['price'])

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

def get_box_plot_data(labels, bp):
    rows_list = []

    for i in range(len(labels)):
        dict1 = {}
        dict1['label'] = labels[i]
        dict1['lower_whisker'] = bp['whiskers'][i*2].get_ydata()[1]
        dict1['lower_quartile'] = bp['boxes'][i].get_ydata()[1]
        dict1['median'] = bp['medians'][i].get_ydata()[1]
        dict1['upper_quartile'] = bp['boxes'][i].get_ydata()[2]
        dict1['upper_whisker'] = bp['whiskers'][(i*2)+1].get_ydata()[1]
        rows_list.append(dict1)

    return pd.DataFrame(rows_list)

labels = ['price']
bp = plt.boxplot(df[['price']], labels=labels)
print(get_box_plot_data(labels, bp))
plt.show()

# Create a box plot and capture the output to get outlier values
boxplot_output = plt.boxplot(df['price'], showfliers=True)

# Extract outlier values
outlier_values = [flier.get_ydata() for flier in boxplot_output['fliers']]

print(len(outlier_values[0]))

# Create a box plot and capture the output to get outlier values
boxplot_output = plt.boxplot(df['volume'], showfliers=True)

# Extract outlier values
outlier_values = [flier.get_ydata() for flier in boxplot_output['fliers']]

print(len(outlier_values[0]))